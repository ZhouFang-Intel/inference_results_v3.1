make[1]: Entering directory '/work'
[2023-12-15 17:42:04,036 main.py:230 INFO] Detected system ID: KnownSystem.ACC_H100
[2023-12-15 17:42:09,364 generate_engines.py:172 INFO] Building engines for bert benchmark in Offline scenario...
[2023-12-15 17:42:09,388 bert_var_seqlen.py:87 INFO] Using workspace size: 7516192768
[12/15/2023-17:42:09] [TRT] [I] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 44, GPU 928 (MiB)
[12/15/2023-17:42:22] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4333, GPU +1150, now: CPU 4482, GPU 2078 (MiB)
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 0) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 3) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l0_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 4) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l0_fc_mid_out and (Unnamed Layer* 6) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 11) [ElementWise]_output and (Unnamed Layer* 7) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 13) [ElementWise]_output and (Unnamed Layer* 8) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 15) [Activation]_output and (Unnamed Layer* 9) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 16) [ElementWise]_output and (Unnamed Layer* 10) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l0_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 19) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 4) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 23) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l1_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 24) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l1_fc_mid_out and (Unnamed Layer* 26) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 31) [ElementWise]_output and (Unnamed Layer* 27) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 33) [ElementWise]_output and (Unnamed Layer* 28) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 35) [Activation]_output and (Unnamed Layer* 29) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 36) [ElementWise]_output and (Unnamed Layer* 30) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l1_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 39) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 24) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 43) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l2_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 44) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l2_fc_mid_out and (Unnamed Layer* 46) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 51) [ElementWise]_output and (Unnamed Layer* 47) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 53) [ElementWise]_output and (Unnamed Layer* 48) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 55) [Activation]_output and (Unnamed Layer* 49) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 56) [ElementWise]_output and (Unnamed Layer* 50) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l2_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 59) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 44) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 63) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l3_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 64) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l3_fc_mid_out and (Unnamed Layer* 66) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 71) [ElementWise]_output and (Unnamed Layer* 67) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 73) [ElementWise]_output and (Unnamed Layer* 68) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 75) [Activation]_output and (Unnamed Layer* 69) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 76) [ElementWise]_output and (Unnamed Layer* 70) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l3_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 79) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 64) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 83) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l4_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 84) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l4_fc_mid_out and (Unnamed Layer* 86) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 91) [ElementWise]_output and (Unnamed Layer* 87) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 93) [ElementWise]_output and (Unnamed Layer* 88) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 95) [Activation]_output and (Unnamed Layer* 89) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 96) [ElementWise]_output and (Unnamed Layer* 90) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l4_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 99) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 84) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 103) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l5_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 104) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l5_fc_mid_out and (Unnamed Layer* 106) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 111) [ElementWise]_output and (Unnamed Layer* 107) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 113) [ElementWise]_output and (Unnamed Layer* 108) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 115) [Activation]_output and (Unnamed Layer* 109) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 116) [ElementWise]_output and (Unnamed Layer* 110) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l5_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 119) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 104) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 123) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l6_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 124) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l6_fc_mid_out and (Unnamed Layer* 126) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 131) [ElementWise]_output and (Unnamed Layer* 127) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 133) [ElementWise]_output and (Unnamed Layer* 128) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 135) [Activation]_output and (Unnamed Layer* 129) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 136) [ElementWise]_output and (Unnamed Layer* 130) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l6_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 139) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 124) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 143) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l7_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 144) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l7_fc_mid_out and (Unnamed Layer* 146) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 151) [ElementWise]_output and (Unnamed Layer* 147) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 153) [ElementWise]_output and (Unnamed Layer* 148) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 155) [Activation]_output and (Unnamed Layer* 149) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 156) [ElementWise]_output and (Unnamed Layer* 150) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l7_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 159) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 144) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 163) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l8_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 164) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l8_fc_mid_out and (Unnamed Layer* 166) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 171) [ElementWise]_output and (Unnamed Layer* 167) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 173) [ElementWise]_output and (Unnamed Layer* 168) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 175) [Activation]_output and (Unnamed Layer* 169) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 176) [ElementWise]_output and (Unnamed Layer* 170) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l8_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 179) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 164) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 183) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l9_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 184) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l9_fc_mid_out and (Unnamed Layer* 186) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 191) [ElementWise]_output and (Unnamed Layer* 187) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 193) [ElementWise]_output and (Unnamed Layer* 188) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 195) [Activation]_output and (Unnamed Layer* 189) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 196) [ElementWise]_output and (Unnamed Layer* 190) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l9_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 199) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 184) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 203) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l10_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 204) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l10_fc_mid_out and (Unnamed Layer* 206) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 211) [ElementWise]_output and (Unnamed Layer* 207) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 213) [ElementWise]_output and (Unnamed Layer* 208) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 215) [Activation]_output and (Unnamed Layer* 209) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 216) [ElementWise]_output and (Unnamed Layer* 210) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l10_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 219) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 204) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 223) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l11_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 224) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l11_fc_mid_out and (Unnamed Layer* 226) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 231) [ElementWise]_output and (Unnamed Layer* 227) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 233) [ElementWise]_output and (Unnamed Layer* 228) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 235) [Activation]_output and (Unnamed Layer* 229) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 236) [ElementWise]_output and (Unnamed Layer* 230) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l11_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 239) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 224) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 243) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l12_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 244) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l12_fc_mid_out and (Unnamed Layer* 246) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 251) [ElementWise]_output and (Unnamed Layer* 247) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 253) [ElementWise]_output and (Unnamed Layer* 248) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 255) [Activation]_output and (Unnamed Layer* 249) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 256) [ElementWise]_output and (Unnamed Layer* 250) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l12_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 259) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 244) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 263) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l13_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 264) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l13_fc_mid_out and (Unnamed Layer* 266) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 271) [ElementWise]_output and (Unnamed Layer* 267) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 273) [ElementWise]_output and (Unnamed Layer* 268) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 275) [Activation]_output and (Unnamed Layer* 269) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 276) [ElementWise]_output and (Unnamed Layer* 270) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l13_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 279) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 264) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 283) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l14_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 284) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l14_fc_mid_out and (Unnamed Layer* 286) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 291) [ElementWise]_output and (Unnamed Layer* 287) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 293) [ElementWise]_output and (Unnamed Layer* 288) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 295) [Activation]_output and (Unnamed Layer* 289) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 296) [ElementWise]_output and (Unnamed Layer* 290) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l14_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 299) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 284) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 303) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l15_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 304) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l15_fc_mid_out and (Unnamed Layer* 306) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 311) [ElementWise]_output and (Unnamed Layer* 307) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 313) [ElementWise]_output and (Unnamed Layer* 308) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 315) [Activation]_output and (Unnamed Layer* 309) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 316) [ElementWise]_output and (Unnamed Layer* 310) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l15_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 319) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 304) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 323) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l16_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 324) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs l16_fc_mid_out and (Unnamed Layer* 326) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 331) [ElementWise]_output and (Unnamed Layer* 327) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 333) [ElementWise]_output and (Unnamed Layer* 328) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 335) [Activation]_output and (Unnamed Layer* 329) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 336) [ElementWise]_output and (Unnamed Layer* 330) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l16_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 339) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 324) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:25] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 343) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l17_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 344) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs l17_fc_mid_out and (Unnamed Layer* 346) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 351) [ElementWise]_output and (Unnamed Layer* 347) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 353) [ElementWise]_output and (Unnamed Layer* 348) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 355) [Activation]_output and (Unnamed Layer* 349) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 356) [ElementWise]_output and (Unnamed Layer* 350) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l17_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 359) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 344) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 363) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l18_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 364) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs l18_fc_mid_out and (Unnamed Layer* 366) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 371) [ElementWise]_output and (Unnamed Layer* 367) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 373) [ElementWise]_output and (Unnamed Layer* 368) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 375) [Activation]_output and (Unnamed Layer* 369) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 376) [ElementWise]_output and (Unnamed Layer* 370) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l18_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 379) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 364) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 383) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l19_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 384) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs l19_fc_mid_out and (Unnamed Layer* 386) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 391) [ElementWise]_output and (Unnamed Layer* 387) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 393) [ElementWise]_output and (Unnamed Layer* 388) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 395) [Activation]_output and (Unnamed Layer* 389) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 396) [ElementWise]_output and (Unnamed Layer* 390) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l19_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 399) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 384) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 403) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l20_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 404) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs l20_fc_mid_out and (Unnamed Layer* 406) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 411) [ElementWise]_output and (Unnamed Layer* 407) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 413) [ElementWise]_output and (Unnamed Layer* 408) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 415) [Activation]_output and (Unnamed Layer* 409) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 416) [ElementWise]_output and (Unnamed Layer* 410) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l20_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 419) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 404) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 423) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l21_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 424) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs l21_fc_mid_out and (Unnamed Layer* 426) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 431) [ElementWise]_output and (Unnamed Layer* 427) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 433) [ElementWise]_output and (Unnamed Layer* 428) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 435) [Activation]_output and (Unnamed Layer* 429) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 436) [ElementWise]_output and (Unnamed Layer* 430) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l21_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 439) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 424) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 443) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l22_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 444) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs l22_fc_mid_out and (Unnamed Layer* 446) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 451) [ElementWise]_output and (Unnamed Layer* 447) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 453) [ElementWise]_output and (Unnamed Layer* 448) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 455) [Activation]_output and (Unnamed Layer* 449) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 456) [ElementWise]_output and (Unnamed Layer* 450) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l22_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 459) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 444) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 463) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l23_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 464) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs l23_fc_mid_out and (Unnamed Layer* 466) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 471) [ElementWise]_output and (Unnamed Layer* 467) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 473) [ElementWise]_output and (Unnamed Layer* 468) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 475) [Activation]_output and (Unnamed Layer* 469) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 476) [ElementWise]_output and (Unnamed Layer* 470) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l23_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 479) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:42:26] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 464) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[2023-12-15 17:42:26,078 bert_var_seqlen.py:232 INFO] Building ./build/engines/ACC_H100/bert/Offline/bert-Offline-gpu-int8_S_384_B_1280_P_2_vs.custom_k_99_MaxP.plan
[12/15/2023-17:42:26] [TRT] [W] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor input_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor segment_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor cu_seqlens, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor max_seqlen, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l0_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 6) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 7) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 8) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 9) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 10) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 11) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 12) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 13) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 14) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 15) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 16) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 17) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l1_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 26) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 27) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 28) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 29) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 30) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 31) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 32) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 33) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 34) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 35) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 36) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 37) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l2_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 46) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 47) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 48) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 49) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 50) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 51) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 52) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 53) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 54) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 55) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 56) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 57) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l3_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 66) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 67) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 68) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 69) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 70) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 71) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 72) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 73) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 74) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 75) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 76) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 77) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l4_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 86) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 87) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 88) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 89) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 90) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 91) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 92) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 93) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 94) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 95) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 96) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 97) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l5_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 106) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 107) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 108) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 109) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 110) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 111) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 112) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 113) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 114) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 115) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 116) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 117) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l6_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 126) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 127) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 128) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 129) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 130) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 131) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 132) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 133) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 134) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 135) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 136) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 137) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l7_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 146) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 147) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 148) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 149) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 150) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 151) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 152) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 153) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 154) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 155) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 156) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 157) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l8_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 166) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 167) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 168) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 169) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 170) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 171) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 172) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 173) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 174) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 175) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 176) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 177) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l9_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 186) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 187) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 188) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 189) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 190) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 191) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 192) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 193) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 194) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 195) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 196) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 197) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l10_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 206) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 207) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 208) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 209) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 210) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 211) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 212) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 213) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 214) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 215) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 216) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 217) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l11_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 226) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 227) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 228) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 229) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 230) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 231) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 232) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 233) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 234) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 235) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 236) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 237) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l12_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 246) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 247) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 248) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 249) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 250) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 251) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 252) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 253) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 254) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 255) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 256) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 257) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l13_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 266) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 267) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 268) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 269) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 270) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 271) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 272) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 273) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 274) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 275) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 276) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 277) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l14_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 286) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 287) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 288) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 289) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 290) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 291) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 292) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 293) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 294) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 295) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 296) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 297) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l15_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 306) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 307) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 308) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 309) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 310) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 311) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 312) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 313) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 314) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 315) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 316) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 317) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l16_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 326) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 327) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 328) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 329) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 330) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 331) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 332) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 333) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 334) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 335) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 336) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 337) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l17_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 346) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 347) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 348) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 349) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 350) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 351) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 352) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 353) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 354) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 355) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 356) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 357) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l18_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 366) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 367) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 368) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 369) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 370) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 371) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 372) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 373) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 374) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 375) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 376) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 377) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l19_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 386) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 387) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 388) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 389) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 390) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 391) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 392) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 393) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 394) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 395) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 396) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 397) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l20_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 406) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 407) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 408) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 409) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 410) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 411) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 412) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 413) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 414) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 415) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 416) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 417) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l21_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 426) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 427) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 428) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 429) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 430) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 431) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 432) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 433) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 434) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 435) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 436) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 437) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l22_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 446) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 447) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 448) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 449) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 450) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 451) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 452) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 453) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 454) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 455) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 456) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 457) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor l23_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 466) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 467) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 468) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 469) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 470) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 471) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 472) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 473) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 474) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 475) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 476) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:26] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 477) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:42:28] [TRT] [I] Graph optimization time: 1.90113 seconds.
[12/15/2023-17:42:28] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +64, now: CPU 6190, GPU 2268 (MiB)
[12/15/2023-17:42:28] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +2, GPU +70, now: CPU 6192, GPU 2338 (MiB)
[12/15/2023-17:42:28] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-17:43:08] [TRT] [I] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
[12/15/2023-17:49:02] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.
[12/15/2023-17:49:03] [TRT] [I] Detected 4 inputs and 1 output network tensors.
[12/15/2023-17:49:23] [TRT] [I] Total Host Persistent Memory: 704352
[12/15/2023-17:49:23] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-17:49:23] [TRT] [I] Total Scratch Memory: 512
[12/15/2023-17:49:23] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 415 steps to complete.
[12/15/2023-17:49:23] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 11.3831ms to assign 8 blocks to 415 nodes requiring 3523217408 bytes.
[12/15/2023-17:49:23] [TRT] [I] Total Activation Memory: 3523215872
[12/15/2023-17:50:08] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.
[12/15/2023-17:50:08] [TRT] [I] Detected 4 inputs and 1 output network tensors.
[12/15/2023-17:50:27] [TRT] [I] Total Host Persistent Memory: 704352
[12/15/2023-17:50:27] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-17:50:27] [TRT] [I] Total Scratch Memory: 512
[12/15/2023-17:50:27] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 415 steps to complete.
[12/15/2023-17:50:27] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 11.3268ms to assign 8 blocks to 415 nodes requiring 3523217408 bytes.
[12/15/2023-17:50:27] [TRT] [I] Total Activation Memory: 3523217408
[12/15/2023-17:50:27] [TRT] [I] Total Weights Memory: 305014280
[12/15/2023-17:50:27] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 6452, GPU 2884 (MiB)
[12/15/2023-17:50:27] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 6452, GPU 2948 (MiB)
[12/15/2023-17:50:27] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 32 MiB, GPU 15378 MiB
[12/15/2023-17:50:28] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +295, now: CPU 0, GPU 295 (MiB)
[12/15/2023-17:50:28] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 7520 MiB
[2023-12-15 17:50:30,862 generate_engines.py:176 INFO] Finished building engines for bert benchmark in Offline scenario.
model_path: build/models/bert/bert_large_v1_1_fake_quant.onnx
Time taken to generate engines: 499.2977523803711 seconds
[2023-12-15 17:50:34,902 generate_engines.py:172 INFO] Building engines for bert benchmark in Server scenario...
[2023-12-15 17:50:34,929 bert_var_seqlen.py:87 INFO] Using workspace size: 7516192768
[12/15/2023-17:50:34] [TRT] [I] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 44, GPU 928 (MiB)
[12/15/2023-17:50:47] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4333, GPU +1150, now: CPU 4482, GPU 2078 (MiB)
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 0) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 3) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l0_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 4) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l0_fc_mid_out and (Unnamed Layer* 6) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 11) [ElementWise]_output and (Unnamed Layer* 7) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 13) [ElementWise]_output and (Unnamed Layer* 8) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 15) [Activation]_output and (Unnamed Layer* 9) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 16) [ElementWise]_output and (Unnamed Layer* 10) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l0_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 19) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 4) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 23) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l1_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 24) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l1_fc_mid_out and (Unnamed Layer* 26) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 31) [ElementWise]_output and (Unnamed Layer* 27) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 33) [ElementWise]_output and (Unnamed Layer* 28) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 35) [Activation]_output and (Unnamed Layer* 29) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 36) [ElementWise]_output and (Unnamed Layer* 30) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l1_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 39) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 24) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 43) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l2_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 44) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l2_fc_mid_out and (Unnamed Layer* 46) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 51) [ElementWise]_output and (Unnamed Layer* 47) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 53) [ElementWise]_output and (Unnamed Layer* 48) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 55) [Activation]_output and (Unnamed Layer* 49) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 56) [ElementWise]_output and (Unnamed Layer* 50) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l2_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 59) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 44) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 63) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l3_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 64) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l3_fc_mid_out and (Unnamed Layer* 66) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 71) [ElementWise]_output and (Unnamed Layer* 67) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 73) [ElementWise]_output and (Unnamed Layer* 68) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 75) [Activation]_output and (Unnamed Layer* 69) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 76) [ElementWise]_output and (Unnamed Layer* 70) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l3_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 79) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 64) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 83) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l4_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 84) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l4_fc_mid_out and (Unnamed Layer* 86) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 91) [ElementWise]_output and (Unnamed Layer* 87) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 93) [ElementWise]_output and (Unnamed Layer* 88) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 95) [Activation]_output and (Unnamed Layer* 89) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 96) [ElementWise]_output and (Unnamed Layer* 90) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l4_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 99) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 84) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 103) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l5_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 104) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l5_fc_mid_out and (Unnamed Layer* 106) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 111) [ElementWise]_output and (Unnamed Layer* 107) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 113) [ElementWise]_output and (Unnamed Layer* 108) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 115) [Activation]_output and (Unnamed Layer* 109) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 116) [ElementWise]_output and (Unnamed Layer* 110) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l5_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 119) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 104) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 123) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l6_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 124) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l6_fc_mid_out and (Unnamed Layer* 126) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 131) [ElementWise]_output and (Unnamed Layer* 127) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 133) [ElementWise]_output and (Unnamed Layer* 128) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 135) [Activation]_output and (Unnamed Layer* 129) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 136) [ElementWise]_output and (Unnamed Layer* 130) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l6_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 139) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 124) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 143) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l7_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 144) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l7_fc_mid_out and (Unnamed Layer* 146) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 151) [ElementWise]_output and (Unnamed Layer* 147) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 153) [ElementWise]_output and (Unnamed Layer* 148) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 155) [Activation]_output and (Unnamed Layer* 149) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 156) [ElementWise]_output and (Unnamed Layer* 150) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l7_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 159) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 144) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 163) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l8_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 164) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l8_fc_mid_out and (Unnamed Layer* 166) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 171) [ElementWise]_output and (Unnamed Layer* 167) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 173) [ElementWise]_output and (Unnamed Layer* 168) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 175) [Activation]_output and (Unnamed Layer* 169) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 176) [ElementWise]_output and (Unnamed Layer* 170) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l8_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 179) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 164) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 183) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l9_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 184) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l9_fc_mid_out and (Unnamed Layer* 186) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 191) [ElementWise]_output and (Unnamed Layer* 187) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 193) [ElementWise]_output and (Unnamed Layer* 188) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 195) [Activation]_output and (Unnamed Layer* 189) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 196) [ElementWise]_output and (Unnamed Layer* 190) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l9_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 199) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 184) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 203) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l10_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 204) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l10_fc_mid_out and (Unnamed Layer* 206) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 211) [ElementWise]_output and (Unnamed Layer* 207) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 213) [ElementWise]_output and (Unnamed Layer* 208) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 215) [Activation]_output and (Unnamed Layer* 209) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 216) [ElementWise]_output and (Unnamed Layer* 210) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l10_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 219) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 204) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 223) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l11_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 224) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l11_fc_mid_out and (Unnamed Layer* 226) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 231) [ElementWise]_output and (Unnamed Layer* 227) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 233) [ElementWise]_output and (Unnamed Layer* 228) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 235) [Activation]_output and (Unnamed Layer* 229) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 236) [ElementWise]_output and (Unnamed Layer* 230) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l11_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 239) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 224) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 243) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l12_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 244) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l12_fc_mid_out and (Unnamed Layer* 246) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 251) [ElementWise]_output and (Unnamed Layer* 247) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 253) [ElementWise]_output and (Unnamed Layer* 248) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 255) [Activation]_output and (Unnamed Layer* 249) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 256) [ElementWise]_output and (Unnamed Layer* 250) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l12_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 259) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 244) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 263) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l13_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 264) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l13_fc_mid_out and (Unnamed Layer* 266) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 271) [ElementWise]_output and (Unnamed Layer* 267) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 273) [ElementWise]_output and (Unnamed Layer* 268) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 275) [Activation]_output and (Unnamed Layer* 269) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 276) [ElementWise]_output and (Unnamed Layer* 270) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l13_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 279) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 264) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 283) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l14_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 284) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l14_fc_mid_out and (Unnamed Layer* 286) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 291) [ElementWise]_output and (Unnamed Layer* 287) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 293) [ElementWise]_output and (Unnamed Layer* 288) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 295) [Activation]_output and (Unnamed Layer* 289) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 296) [ElementWise]_output and (Unnamed Layer* 290) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l14_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 299) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 284) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 303) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l15_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 304) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l15_fc_mid_out and (Unnamed Layer* 306) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 311) [ElementWise]_output and (Unnamed Layer* 307) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 313) [ElementWise]_output and (Unnamed Layer* 308) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 315) [Activation]_output and (Unnamed Layer* 309) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 316) [ElementWise]_output and (Unnamed Layer* 310) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l15_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 319) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 304) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 323) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l16_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 324) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l16_fc_mid_out and (Unnamed Layer* 326) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 331) [ElementWise]_output and (Unnamed Layer* 327) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 333) [ElementWise]_output and (Unnamed Layer* 328) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 335) [Activation]_output and (Unnamed Layer* 329) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 336) [ElementWise]_output and (Unnamed Layer* 330) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l16_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 339) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 324) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 343) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l17_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 344) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l17_fc_mid_out and (Unnamed Layer* 346) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 351) [ElementWise]_output and (Unnamed Layer* 347) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 353) [ElementWise]_output and (Unnamed Layer* 348) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 355) [Activation]_output and (Unnamed Layer* 349) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 356) [ElementWise]_output and (Unnamed Layer* 350) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l17_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 359) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 344) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 363) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l18_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 364) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l18_fc_mid_out and (Unnamed Layer* 366) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 371) [ElementWise]_output and (Unnamed Layer* 367) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 373) [ElementWise]_output and (Unnamed Layer* 368) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 375) [Activation]_output and (Unnamed Layer* 369) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 376) [ElementWise]_output and (Unnamed Layer* 370) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l18_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 379) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 364) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 383) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l19_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 384) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l19_fc_mid_out and (Unnamed Layer* 386) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 391) [ElementWise]_output and (Unnamed Layer* 387) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 393) [ElementWise]_output and (Unnamed Layer* 388) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 395) [Activation]_output and (Unnamed Layer* 389) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 396) [ElementWise]_output and (Unnamed Layer* 390) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l19_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 399) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 384) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 403) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l20_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 404) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l20_fc_mid_out and (Unnamed Layer* 406) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 411) [ElementWise]_output and (Unnamed Layer* 407) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 413) [ElementWise]_output and (Unnamed Layer* 408) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 415) [Activation]_output and (Unnamed Layer* 409) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 416) [ElementWise]_output and (Unnamed Layer* 410) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l20_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 419) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 404) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 423) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l21_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 424) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l21_fc_mid_out and (Unnamed Layer* 426) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 431) [ElementWise]_output and (Unnamed Layer* 427) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 433) [ElementWise]_output and (Unnamed Layer* 428) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 435) [Activation]_output and (Unnamed Layer* 429) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 436) [ElementWise]_output and (Unnamed Layer* 430) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l21_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 439) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 424) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 443) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l22_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 444) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l22_fc_mid_out and (Unnamed Layer* 446) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 451) [ElementWise]_output and (Unnamed Layer* 447) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 453) [ElementWise]_output and (Unnamed Layer* 448) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 455) [Activation]_output and (Unnamed Layer* 449) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 456) [ElementWise]_output and (Unnamed Layer* 450) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l22_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 459) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 444) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [I] Using default for use_int8_scale_max: true
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 463) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l23_attention_fc_aout. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 464) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs l23_fc_mid_out and (Unnamed Layer* 466) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 471) [ElementWise]_output and (Unnamed Layer* 467) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 473) [ElementWise]_output and (Unnamed Layer* 468) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 475) [Activation]_output and (Unnamed Layer* 469) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] IElementWiseLayer with inputs (Unnamed Layer* 476) [ElementWise]_output and (Unnamed Layer* 470) [Constant]_output: first input has type Half but second input has type Float.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: l23_gelu_out. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 479) [Convolution]_output. This will have no effect unless this tensor is marked as an output.
[12/15/2023-17:50:51] [TRT] [W] ITensor::setType(Int8) was called on non I/O tensor: (Unnamed Layer* 464) [PluginV2DynamicExt]_output. This will have no effect unless this tensor is marked as an output.
[2023-12-15 17:50:51,556 bert_var_seqlen.py:232 INFO] Building ./build/engines/ACC_H100/bert/Server/bert-Server-gpu-int8_S_384_B_256_P_2_vs.custom_k_99_MaxP.plan
[12/15/2023-17:50:51] [TRT] [W] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor input_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor segment_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor cu_seqlens, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor max_seqlen, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l0_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 6) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 7) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 8) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 9) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 10) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 11) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 12) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 13) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 14) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 15) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 16) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 17) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l1_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 26) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 27) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 28) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 29) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 30) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 31) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 32) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 33) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 34) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 35) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 36) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 37) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l2_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 46) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 47) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 48) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 49) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 50) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 51) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 52) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 53) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 54) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 55) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 56) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 57) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l3_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 66) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 67) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 68) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 69) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 70) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 71) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 72) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 73) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 74) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 75) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 76) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 77) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l4_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 86) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 87) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 88) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 89) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 90) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 91) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 92) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 93) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 94) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 95) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 96) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 97) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l5_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 106) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 107) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 108) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 109) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 110) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 111) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 112) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 113) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 114) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 115) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 116) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 117) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l6_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 126) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 127) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 128) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 129) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 130) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 131) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 132) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 133) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 134) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 135) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 136) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 137) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l7_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 146) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 147) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 148) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 149) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 150) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 151) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 152) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 153) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 154) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 155) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 156) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 157) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l8_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 166) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 167) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 168) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 169) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 170) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 171) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 172) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 173) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 174) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 175) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 176) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 177) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l9_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 186) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 187) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 188) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 189) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 190) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 191) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 192) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 193) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 194) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 195) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 196) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 197) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l10_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 206) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 207) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 208) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 209) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 210) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 211) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 212) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 213) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 214) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 215) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 216) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 217) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l11_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 226) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 227) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 228) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 229) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 230) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 231) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 232) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 233) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 234) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 235) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 236) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 237) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l12_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 246) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 247) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 248) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 249) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 250) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 251) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 252) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 253) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 254) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 255) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 256) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 257) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l13_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 266) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 267) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 268) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 269) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 270) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 271) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 272) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 273) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 274) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 275) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 276) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 277) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l14_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 286) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 287) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 288) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 289) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 290) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 291) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 292) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 293) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 294) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 295) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 296) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 297) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l15_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 306) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 307) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 308) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 309) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 310) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 311) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 312) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 313) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 314) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 315) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 316) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 317) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l16_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 326) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 327) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 328) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 329) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 330) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 331) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 332) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 333) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 334) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 335) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 336) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 337) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l17_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 346) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 347) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 348) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 349) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 350) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 351) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 352) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 353) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 354) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 355) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 356) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 357) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l18_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 366) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 367) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 368) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 369) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 370) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 371) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 372) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 373) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 374) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 375) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 376) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 377) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l19_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 386) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 387) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 388) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 389) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 390) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 391) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 392) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 393) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 394) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 395) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 396) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 397) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l20_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 406) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 407) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 408) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 409) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 410) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 411) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 412) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 413) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 414) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 415) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 416) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 417) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l21_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 426) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 427) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 428) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 429) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 430) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 431) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 432) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 433) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 434) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 435) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 436) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 437) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l22_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 446) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 447) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 448) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 449) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 450) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 451) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 452) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 453) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 454) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 455) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 456) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 457) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor l23_fc_mid_out, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 466) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 467) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 468) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 469) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 470) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 471) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 472) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 473) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 474) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 475) [Activation]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 476) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:51] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 477) [ElementWise]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-17:50:53] [TRT] [I] Graph optimization time: 1.87178 seconds.
[12/15/2023-17:50:53] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +64, now: CPU 6190, GPU 2268 (MiB)
[12/15/2023-17:50:53] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +2, GPU +70, now: CPU 6192, GPU 2338 (MiB)
[12/15/2023-17:50:53] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-17:51:20] [TRT] [I] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
[12/15/2023-17:54:06] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.
[12/15/2023-17:54:06] [TRT] [I] Detected 4 inputs and 1 output network tensors.
[12/15/2023-17:54:25] [TRT] [I] Total Host Persistent Memory: 704352
[12/15/2023-17:54:25] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-17:54:25] [TRT] [I] Total Scratch Memory: 512
[12/15/2023-17:54:25] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 415 steps to complete.
[12/15/2023-17:54:25] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 10.2585ms to assign 8 blocks to 415 nodes requiring 704645120 bytes.
[12/15/2023-17:54:25] [TRT] [I] Total Activation Memory: 704643584
[12/15/2023-17:55:09] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.
[12/15/2023-17:55:09] [TRT] [I] Detected 4 inputs and 1 output network tensors.
[12/15/2023-17:55:27] [TRT] [I] Total Host Persistent Memory: 704352
[12/15/2023-17:55:27] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-17:55:27] [TRT] [I] Total Scratch Memory: 512
[12/15/2023-17:55:27] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 415 steps to complete.
[12/15/2023-17:55:27] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 10.3103ms to assign 8 blocks to 415 nodes requiring 704645120 bytes.
[12/15/2023-17:55:27] [TRT] [I] Total Activation Memory: 704645120
[12/15/2023-17:55:27] [TRT] [I] Total Weights Memory: 305014280
[12/15/2023-17:55:27] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 6453, GPU 2884 (MiB)
[12/15/2023-17:55:27] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 6453, GPU 2948 (MiB)
[12/15/2023-17:55:28] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 32 MiB, GPU 8463 MiB
[12/15/2023-17:55:28] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +295, now: CPU 0, GPU 295 (MiB)
[12/15/2023-17:55:28] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 7520 MiB
[2023-12-15 17:55:30,744 generate_engines.py:176 INFO] Finished building engines for bert benchmark in Server scenario.
model_path: build/models/bert/bert_large_v1_1_fake_quant.onnx
Time taken to generate engines: 293.7015497684479 seconds
[2023-12-15 17:55:34,363 generate_engines.py:172 INFO] Building engines for gptj benchmark in Offline scenario...
[12/15/2023-17:55:34] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 44, GPU 928 (MiB)
[12/15/2023-17:55:46] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4332, GPU +1150, now: CPU 4481, GPU 2078 (MiB)
[2023-12-15 17:55:48,447 gptj6b.py:103 INFO] Building GPTJ engine in ./build/engines/ACC_H100/gptj/Offline, use_fp8: True command: python build/TRTLLM/examples/gptj/build.py --dtype=float16 --use_gpt_attention_plugin=float16 --use_gemm_plugin=float16 --max_batch_size=32 --max_input_len=1919 --max_output_len=128 --vocab_size=50401 --max_beam_width=4 --output_dir=./build/engines/ACC_H100/gptj/Offline --model_dir=build/models/GPTJ-6B/checkpoint-final --enable_context_fmha --enable_two_optimization_profiles --enable_fp8 --quantized_fp8_model_path=/opt/GPTJ-07142023.pth --fp8_kv_cache
[2023-12-15 18:21:14,039 gptj6b.py:122 INFO] Engine built complete and took 1525.592030763626s. Stored at ./build/engines/ACC_H100/gptj/Offline/gptj-Offline-gpu-b32-fp16.custom_k_99_MaxP.plan
[2023-12-15 18:21:14,040 generate_engines.py:176 INFO] Finished building engines for gptj benchmark in Offline scenario.
Time taken to generate engines: 1539.6759548187256 seconds
[2023-12-15 18:21:19,806 generate_engines.py:172 INFO] Building engines for gptj benchmark in Server scenario...
[12/15/2023-18:21:19] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 44, GPU 928 (MiB)
[12/15/2023-18:21:32] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4332, GPU +1150, now: CPU 4481, GPU 2078 (MiB)
[2023-12-15 18:21:34,525 gptj6b.py:103 INFO] Building GPTJ engine in ./build/engines/ACC_H100/gptj/Server, use_fp8: True command: python build/TRTLLM/examples/gptj/build.py --dtype=float16 --use_gpt_attention_plugin=float16 --use_gemm_plugin=float16 --max_batch_size=32 --max_input_len=1919 --max_output_len=128 --vocab_size=50401 --max_beam_width=4 --output_dir=./build/engines/ACC_H100/gptj/Server --model_dir=build/models/GPTJ-6B/checkpoint-final --enable_context_fmha --enable_two_optimization_profiles --enable_fp8 --quantized_fp8_model_path=/opt/GPTJ-07142023.pth --fp8_kv_cache
[2023-12-15 18:47:58,239 gptj6b.py:122 INFO] Engine built complete and took 1583.7139191627502s. Stored at ./build/engines/ACC_H100/gptj/Server/gptj-Server-gpu-b32-fp16.custom_k_99_MaxP.plan
[2023-12-15 18:47:58,240 generate_engines.py:176 INFO] Finished building engines for gptj benchmark in Server scenario.
Time taken to generate engines: 1598.4332628250122 seconds
[2023-12-15 18:48:04,241 generate_engines.py:172 INFO] Building engines for resnet50 benchmark in Offline scenario...
[12/15/2023-18:48:04] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 45, GPU 928 (MiB)
[12/15/2023-18:48:17] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4333, GPU +1150, now: CPU 4483, GPU 2078 (MiB)
[2023-12-15 18:48:18,251 rn50_graphsurgeon.py:528 INFO] Adding Squeeze
[2023-12-15 18:48:18,251 rn50_graphsurgeon.py:563 INFO] Adding Conv layer, instead of FC
[2023-12-15 18:48:18,262 rn50_graphsurgeon.py:584 INFO] Adding TopK layer
[2023-12-15 18:48:18,262 rn50_graphsurgeon.py:601 INFO] Adding Cast Layer
[2023-12-15 18:48:18,262 rn50_graphsurgeon.py:620 INFO] Removing obsolete layers
[2023-12-15 18:48:18,264 rn50_graphsurgeon.py:410 INFO] Fusing ops in res2_mega
[2023-12-15 18:48:18,267 rn50_graphsurgeon.py:522 INFO] Plugin RES2_FULL_FUSION successful
[2023-12-15 18:48:18,267 rn50_graphsurgeon.py:330 INFO] Replacing all branch2c beta=1 conv with smallk kernel.
[2023-12-15 18:48:18,268 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res3a_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,268 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res3b_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,268 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res3c_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,268 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res3d_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,268 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res4a_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,268 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res4b_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,268 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res4c_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,268 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res4d_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,269 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res4e_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,269 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res4f_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,269 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res5a_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,269 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res5b_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,269 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res5c_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:48:18,271 rn50_graphsurgeon.py:403 INFO] Plugin SmallTileGEMM_TRT fused successful for res3/4/5 branch2c
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: RnRes2FullFusion_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: RnRes2FullFusion_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: RnRes2FullFusion_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:48:18] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:48:18] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:48:18] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[2023-12-15 18:48:18,590 ResNet50.py:258 INFO] Unmarking output: topk_layer_output_value
[2023-12-15 18:48:18,591 builder.py:327 INFO] Building engine to build/engines/ACC_H100/resnet50/Offline/resnet50-Offline-gpu-b2048-int8.lwis_k_99_MaxP.plan
[2023-12-15 18:48:18,591 builder.py:333 INFO] Building optimization profiles.
[12/15/2023-18:48:18] [TRT] [I] Graph optimization time: 0.00153256 seconds.
[12/15/2023-18:48:18] [TRT] [I] Reading Calibration Cache for calibrator: EntropyCalibration2
[12/15/2023-18:48:18] [TRT] [I] Generated calibration scales using calibration cache. Make sure that calibration cache has latest scales.
[12/15/2023-18:48:18] [TRT] [I] To regenerate calibration cache, please delete the existing one. TensorRT will generate a new calibration cache.
[12/15/2023-18:48:18] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 75) [TopK]_output_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-18:48:18] [TRT] [W] Missing scale and zero-point for tensor topk_layer_output_index_i64, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-18:48:18] [TRT] [W] Missing scale and zero-point for tensor topk_layer_output_index, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-18:48:18] [TRT] [I] Graph optimization time: 0.00438632 seconds.
[12/15/2023-18:48:18] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +64, now: CPU 4992, GPU 2146 (MiB)
[12/15/2023-18:48:18] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +70, now: CPU 4993, GPU 2216 (MiB)
[12/15/2023-18:48:18] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:48:38] [TRT] [I] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
[12/15/2023-18:51:14] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[12/15/2023-18:51:19] [TRT] [I] Total Host Persistent Memory: 222672
[12/15/2023-18:51:19] [TRT] [I] Total Device Persistent Memory: 22528
[12/15/2023-18:51:19] [TRT] [I] Total Scratch Memory: 7233536
[12/15/2023-18:51:19] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 80 steps to complete.
[12/15/2023-18:51:19] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 1.51076ms to assign 4 blocks to 80 nodes requiring 3339714560 bytes.
[12/15/2023-18:51:19] [TRT] [I] Total Activation Memory: 3339714560
[12/15/2023-18:51:25] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[12/15/2023-18:51:28] [TRT] [I] Total Host Persistent Memory: 222672
[12/15/2023-18:51:28] [TRT] [I] Total Device Persistent Memory: 22528
[12/15/2023-18:51:28] [TRT] [I] Total Scratch Memory: 7233536
[12/15/2023-18:51:28] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 80 steps to complete.
[12/15/2023-18:51:28] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 1.48366ms to assign 4 blocks to 80 nodes requiring 3339714560 bytes.
[12/15/2023-18:51:28] [TRT] [I] Total Activation Memory: 3339714560
[12/15/2023-18:51:28] [TRT] [I] Total Weights Memory: 20560648
[12/15/2023-18:51:28] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 5150, GPU 4836 (MiB)
[12/15/2023-18:51:28] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 5150, GPU 4900 (MiB)
[12/15/2023-18:51:28] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 18 MiB, GPU 12545 MiB
[12/15/2023-18:51:29] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +22, now: CPU 0, GPU 22 (MiB)
[12/15/2023-18:51:29] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:51:29,059 builder.py:340 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:51:29,060 builder.py:341 INFO] Layers:
conv1 + scale_conv1 + conv1_relu + pool1
RES2_FULL_FUSION
res3a_branch1
res3a_branch2a + res3a_branch2a_relu
res3a_branch2b + res3a_branch2b_relu
SmallTileGEMM_TRT_res3a_branch2c_conv_residual_relu
res3b_branch2a + res3b_branch2a_relu
res3b_branch2b + res3b_branch2b_relu
SmallTileGEMM_TRT_res3b_branch2c_conv_residual_relu
res3c_branch2a + res3c_branch2a_relu
res3c_branch2b + res3c_branch2b_relu
SmallTileGEMM_TRT_res3c_branch2c_conv_residual_relu
res3d_branch2a + res3d_branch2a_relu
res3d_branch2b + res3d_branch2b_relu
SmallTileGEMM_TRT_res3d_branch2c_conv_residual_relu
res4a_branch1
res4a_branch2a + res4a_branch2a_relu
res4a_branch2b + res4a_branch2b_relu
SmallTileGEMM_TRT_res4a_branch2c_conv_residual_relu
res4b_branch2a + res4b_branch2a_relu
res4b_branch2b + res4b_branch2b_relu
SmallTileGEMM_TRT_res4b_branch2c_conv_residual_relu
res4c_branch2a + res4c_branch2a_relu
res4c_branch2b + res4c_branch2b_relu
SmallTileGEMM_TRT_res4c_branch2c_conv_residual_relu
res4d_branch2a + res4d_branch2a_relu
res4d_branch2b + res4d_branch2b_relu
SmallTileGEMM_TRT_res4d_branch2c_conv_residual_relu
res4e_branch2a + res4e_branch2a_relu
res4e_branch2b + res4e_branch2b_relu
SmallTileGEMM_TRT_res4e_branch2c_conv_residual_relu
res4f_branch2a + res4f_branch2a_relu
res4f_branch2b + res4f_branch2b_relu
SmallTileGEMM_TRT_res4f_branch2c_conv_residual_relu
res5a_branch2a + res5a_branch2a_relu
res5a_branch1
res5a_branch2b + res5a_branch2b_relu
SmallTileGEMM_TRT_res5a_branch2c_conv_residual_relu
res5b_branch2a + res5b_branch2a_relu
res5b_branch2b + res5b_branch2b_relu
SmallTileGEMM_TRT_res5b_branch2c_conv_residual_relu
res5c_branch2a + res5c_branch2a_relu
res5c_branch2b + res5c_branch2b_relu
SmallTileGEMM_TRT_res5c_branch2c_conv_residual_relu
squeeze_replaced
fc_replaced
topk_layer
cast_layer
conv1 + scale_conv1 + conv1_relu + pool1 [profile 1]
RES2_FULL_FUSION [profile 1]
res3a_branch1 [profile 1]
res3a_branch2a + res3a_branch2a_relu [profile 1]
res3a_branch2b + res3a_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res3a_branch2c_conv_residual_relu [profile 1]
res3b_branch2a + res3b_branch2a_relu [profile 1]
res3b_branch2b + res3b_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res3b_branch2c_conv_residual_relu [profile 1]
res3c_branch2a + res3c_branch2a_relu [profile 1]
res3c_branch2b + res3c_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res3c_branch2c_conv_residual_relu [profile 1]
res3d_branch2a + res3d_branch2a_relu [profile 1]
res3d_branch2b + res3d_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res3d_branch2c_conv_residual_relu [profile 1]
res4a_branch1 [profile 1]
res4a_branch2a + res4a_branch2a_relu [profile 1]
res4a_branch2b + res4a_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res4a_branch2c_conv_residual_relu [profile 1]
res4b_branch2a + res4b_branch2a_relu [profile 1]
res4b_branch2b + res4b_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res4b_branch2c_conv_residual_relu [profile 1]
res4c_branch2a + res4c_branch2a_relu [profile 1]
res4c_branch2b + res4c_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res4c_branch2c_conv_residual_relu [profile 1]
res4d_branch2a + res4d_branch2a_relu [profile 1]
res4d_branch2b + res4d_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res4d_branch2c_conv_residual_relu [profile 1]
res4e_branch2a + res4e_branch2a_relu [profile 1]
res4e_branch2b + res4e_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res4e_branch2c_conv_residual_relu [profile 1]
res4f_branch2a + res4f_branch2a_relu [profile 1]
res4f_branch2b + res4f_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res4f_branch2c_conv_residual_relu [profile 1]
res5a_branch2a + res5a_branch2a_relu [profile 1]
res5a_branch1 [profile 1]
res5a_branch2b + res5a_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res5a_branch2c_conv_residual_relu [profile 1]
res5b_branch2a + res5b_branch2a_relu [profile 1]
res5b_branch2b + res5b_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res5b_branch2c_conv_residual_relu [profile 1]
res5c_branch2a + res5c_branch2a_relu [profile 1]
res5c_branch2b + res5c_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res5c_branch2c_conv_residual_relu [profile 1]
squeeze_replaced [profile 1]
fc_replaced [profile 1]
topk_layer [profile 1]
cast_layer [profile 1]

Bindings:
input_tensor_0
topk_layer_output_index
input_tensor_0 [profile 1]
topk_layer_output_index [profile 1]

[2023-12-15 18:51:29,107 generate_engines.py:176 INFO] Finished building engines for resnet50 benchmark in Offline scenario.
Time taken to generate engines: 204.86563229560852 seconds
[2023-12-15 18:51:34,801 generate_engines.py:172 INFO] Building engines for resnet50 benchmark in Server scenario...
[12/15/2023-18:51:34] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 45, GPU 928 (MiB)
[12/15/2023-18:51:47] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4333, GPU +1150, now: CPU 4483, GPU 2078 (MiB)
[2023-12-15 18:51:48,701 rn50_graphsurgeon.py:528 INFO] Adding Squeeze
[2023-12-15 18:51:48,701 rn50_graphsurgeon.py:563 INFO] Adding Conv layer, instead of FC
[2023-12-15 18:51:48,712 rn50_graphsurgeon.py:584 INFO] Adding TopK layer
[2023-12-15 18:51:48,712 rn50_graphsurgeon.py:601 INFO] Adding Cast Layer
[2023-12-15 18:51:48,712 rn50_graphsurgeon.py:620 INFO] Removing obsolete layers
[2023-12-15 18:51:48,714 rn50_graphsurgeon.py:410 INFO] Fusing ops in res2_mega
[2023-12-15 18:51:48,716 rn50_graphsurgeon.py:522 INFO] Plugin RES2_FULL_FUSION successful
[2023-12-15 18:51:48,716 rn50_graphsurgeon.py:330 INFO] Replacing all branch2c beta=1 conv with smallk kernel.
[2023-12-15 18:51:48,717 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res3a_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,717 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res3b_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,717 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res3c_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,717 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res3d_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,717 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res4a_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,717 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res4b_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,717 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res4c_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,717 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res4d_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,717 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res4e_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,718 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res4f_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,718 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res5a_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,718 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res5b_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,718 rn50_graphsurgeon.py:396 INFO] Fusing SmallTileGEMM_TRT_res5c_branch2c_conv_residual_relu with smallk...
[2023-12-15 18:51:48,719 rn50_graphsurgeon.py:403 INFO] Plugin SmallTileGEMM_TRT fused successful for res3/4/5 branch2c
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: RnRes2FullFusion_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: RnRes2FullFusion_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [I] Successfully created plugin: RnRes2FullFusion_TRT
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:51:48] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:48] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:48] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:49] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[12/15/2023-18:51:49] [TRT] [I] No importer registered for op: SmallTileGEMM_TRT. Attempting to import as plugin.
[12/15/2023-18:51:49] [TRT] [I] Searching for plugin: SmallTileGEMM_TRT, plugin_version: 1, plugin_namespace: 
[12/15/2023-18:51:49] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBias not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:49] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasRelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:49] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasGelu not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:49] [TRT] [W] builtin_op_importers.cpp:5421: Attribute epilogueScaleBiasBeta not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.
[12/15/2023-18:51:49] [TRT] [I] Successfully created plugin: SmallTileGEMM_TRT
[2023-12-15 18:51:49,036 ResNet50.py:258 INFO] Unmarking output: topk_layer_output_value
[2023-12-15 18:51:49,037 builder.py:327 INFO] Building engine to build/engines/ACC_H100/resnet50/Server/resnet50-Server-gpu-b128-int8.lwis_k_99_MaxP.plan
[2023-12-15 18:51:49,037 builder.py:333 INFO] Building optimization profiles.
[12/15/2023-18:51:49] [TRT] [I] Graph optimization time: 0.00154409 seconds.
[12/15/2023-18:51:49] [TRT] [I] Reading Calibration Cache for calibrator: EntropyCalibration2
[12/15/2023-18:51:49] [TRT] [I] Generated calibration scales using calibration cache. Make sure that calibration cache has latest scales.
[12/15/2023-18:51:49] [TRT] [I] To regenerate calibration cache, please delete the existing one. TensorRT will generate a new calibration cache.
[12/15/2023-18:51:49] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 75) [TopK]_output_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-18:51:49] [TRT] [W] Missing scale and zero-point for tensor topk_layer_output_index_i64, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-18:51:49] [TRT] [W] Missing scale and zero-point for tensor topk_layer_output_index, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[12/15/2023-18:51:49] [TRT] [I] Graph optimization time: 0.00436655 seconds.
[12/15/2023-18:51:49] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +64, now: CPU 4992, GPU 2146 (MiB)
[12/15/2023-18:51:49] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +70, now: CPU 4993, GPU 2216 (MiB)
[12/15/2023-18:51:49] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:53:04] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[12/15/2023-18:53:09] [TRT] [I] Total Host Persistent Memory: 212432
[12/15/2023-18:53:09] [TRT] [I] Total Device Persistent Memory: 22528
[12/15/2023-18:53:09] [TRT] [I] Total Scratch Memory: 7233536
[12/15/2023-18:53:09] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 61 steps to complete.
[12/15/2023-18:53:09] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.980542ms to assign 4 blocks to 61 nodes requiring 208732160 bytes.
[12/15/2023-18:53:09] [TRT] [I] Total Activation Memory: 208732160
[12/15/2023-18:53:15] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[12/15/2023-18:53:19] [TRT] [I] Total Host Persistent Memory: 212432
[12/15/2023-18:53:19] [TRT] [I] Total Device Persistent Memory: 22528
[12/15/2023-18:53:19] [TRT] [I] Total Scratch Memory: 7233536
[12/15/2023-18:53:19] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 61 steps to complete.
[12/15/2023-18:53:19] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.990919ms to assign 4 blocks to 61 nodes requiring 208732160 bytes.
[12/15/2023-18:53:19] [TRT] [I] Total Activation Memory: 208732160
[12/15/2023-18:53:25] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[12/15/2023-18:53:29] [TRT] [I] Total Host Persistent Memory: 212432
[12/15/2023-18:53:29] [TRT] [I] Total Device Persistent Memory: 22528
[12/15/2023-18:53:29] [TRT] [I] Total Scratch Memory: 7233536
[12/15/2023-18:53:29] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 61 steps to complete.
[12/15/2023-18:53:29] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 1.00466ms to assign 4 blocks to 61 nodes requiring 208732160 bytes.
[12/15/2023-18:53:29] [TRT] [I] Total Activation Memory: 208732160
[12/15/2023-18:53:35] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[12/15/2023-18:53:39] [TRT] [I] Total Host Persistent Memory: 212432
[12/15/2023-18:53:39] [TRT] [I] Total Device Persistent Memory: 22528
[12/15/2023-18:53:39] [TRT] [I] Total Scratch Memory: 7233536
[12/15/2023-18:53:39] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 61 steps to complete.
[12/15/2023-18:53:39] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.98472ms to assign 4 blocks to 61 nodes requiring 208732160 bytes.
[12/15/2023-18:53:39] [TRT] [I] Total Activation Memory: 208732160
[12/15/2023-18:53:39] [TRT] [I] Total Weights Memory: 20560648
[12/15/2023-18:53:39] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 5171, GPU 4836 (MiB)
[12/15/2023-18:53:39] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 5171, GPU 4900 (MiB)
[12/15/2023-18:53:39] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 18 MiB, GPU 981 MiB
[12/15/2023-18:53:39] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +22, now: CPU 0, GPU 22 (MiB)
[12/15/2023-18:53:39] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:53:39,750 builder.py:340 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:53:39,751 builder.py:341 INFO] Layers:
conv1 + scale_conv1 + conv1_relu + pool1
RES2_FULL_FUSION
res3a_branch1
res3a_branch2a + res3a_branch2a_relu
res3a_branch2b + res3a_branch2b_relu
SmallTileGEMM_TRT_res3a_branch2c_conv_residual_relu
res3b_branch2a + res3b_branch2a_relu
res3b_branch2b + res3b_branch2b_relu
SmallTileGEMM_TRT_res3b_branch2c_conv_residual_relu
res3c_branch2a + res3c_branch2a_relu
res3c_branch2b + res3c_branch2b_relu
SmallTileGEMM_TRT_res3c_branch2c_conv_residual_relu
res3d_branch2a + res3d_branch2a_relu
res3d_branch2b + res3d_branch2b_relu
SmallTileGEMM_TRT_res3d_branch2c_conv_residual_relu
res4a_branch1
res4a_branch2a + res4a_branch2a_relu
res4a_branch2b + res4a_branch2b_relu
SmallTileGEMM_TRT_res4a_branch2c_conv_residual_relu
res4b_branch2a + res4b_branch2a_relu
res4b_branch2b + res4b_branch2b_relu
SmallTileGEMM_TRT_res4b_branch2c_conv_residual_relu
res4c_branch2a + res4c_branch2a_relu
res4c_branch2b + res4c_branch2b_relu
SmallTileGEMM_TRT_res4c_branch2c_conv_residual_relu
res4d_branch2a + res4d_branch2a_relu
res4d_branch2b + res4d_branch2b_relu
SmallTileGEMM_TRT_res4d_branch2c_conv_residual_relu
res4e_branch2a + res4e_branch2a_relu
res4e_branch2b + res4e_branch2b_relu
SmallTileGEMM_TRT_res4e_branch2c_conv_residual_relu
res4f_branch2a + res4f_branch2a_relu
res4f_branch2b + res4f_branch2b_relu
SmallTileGEMM_TRT_res4f_branch2c_conv_residual_relu
res5a_branch2a + res5a_branch2a_relu
res5a_branch1
res5a_branch2b + res5a_branch2b_relu
SmallTileGEMM_TRT_res5a_branch2c_conv_residual_relu
res5b_branch2a + res5b_branch2a_relu
res5b_branch2b + res5b_branch2b_relu
SmallTileGEMM_TRT_res5b_branch2c_conv_residual_relu
res5c_branch2a + res5c_branch2a_relu
res5c_branch2b + res5c_branch2b_relu
SmallTileGEMM_TRT_res5c_branch2c_conv_residual_relu
squeeze_replaced
fc_replaced
topk_layer
cast_layer
conv1 + scale_conv1 + conv1_relu + pool1 [profile 1]
RES2_FULL_FUSION [profile 1]
res3a_branch1 [profile 1]
res3a_branch2a + res3a_branch2a_relu [profile 1]
res3a_branch2b + res3a_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res3a_branch2c_conv_residual_relu [profile 1]
res3b_branch2a + res3b_branch2a_relu [profile 1]
res3b_branch2b + res3b_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res3b_branch2c_conv_residual_relu [profile 1]
res3c_branch2a + res3c_branch2a_relu [profile 1]
res3c_branch2b + res3c_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res3c_branch2c_conv_residual_relu [profile 1]
res3d_branch2a + res3d_branch2a_relu [profile 1]
res3d_branch2b + res3d_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res3d_branch2c_conv_residual_relu [profile 1]
res4a_branch1 [profile 1]
res4a_branch2a + res4a_branch2a_relu [profile 1]
res4a_branch2b + res4a_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res4a_branch2c_conv_residual_relu [profile 1]
res4b_branch2a + res4b_branch2a_relu [profile 1]
res4b_branch2b + res4b_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res4b_branch2c_conv_residual_relu [profile 1]
res4c_branch2a + res4c_branch2a_relu [profile 1]
res4c_branch2b + res4c_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res4c_branch2c_conv_residual_relu [profile 1]
res4d_branch2a + res4d_branch2a_relu [profile 1]
res4d_branch2b + res4d_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res4d_branch2c_conv_residual_relu [profile 1]
res4e_branch2a + res4e_branch2a_relu [profile 1]
res4e_branch2b + res4e_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res4e_branch2c_conv_residual_relu [profile 1]
res4f_branch2a + res4f_branch2a_relu [profile 1]
res4f_branch2b + res4f_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res4f_branch2c_conv_residual_relu [profile 1]
res5a_branch2a + res5a_branch2a_relu [profile 1]
res5a_branch1 [profile 1]
res5a_branch2b + res5a_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res5a_branch2c_conv_residual_relu [profile 1]
res5b_branch2a + res5b_branch2a_relu [profile 1]
res5b_branch2b + res5b_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res5b_branch2c_conv_residual_relu [profile 1]
res5c_branch2a + res5c_branch2a_relu [profile 1]
res5c_branch2b + res5c_branch2b_relu [profile 1]
SmallTileGEMM_TRT_res5c_branch2c_conv_residual_relu [profile 1]
squeeze_replaced [profile 1]
fc_replaced [profile 1]
topk_layer [profile 1]
cast_layer [profile 1]
conv1 + scale_conv1 + conv1_relu + pool1 [profile 2]
RES2_FULL_FUSION [profile 2]
res3a_branch1 [profile 2]
res3a_branch2a + res3a_branch2a_relu [profile 2]
res3a_branch2b + res3a_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res3a_branch2c_conv_residual_relu [profile 2]
res3b_branch2a + res3b_branch2a_relu [profile 2]
res3b_branch2b + res3b_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res3b_branch2c_conv_residual_relu [profile 2]
res3c_branch2a + res3c_branch2a_relu [profile 2]
res3c_branch2b + res3c_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res3c_branch2c_conv_residual_relu [profile 2]
res3d_branch2a + res3d_branch2a_relu [profile 2]
res3d_branch2b + res3d_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res3d_branch2c_conv_residual_relu [profile 2]
res4a_branch1 [profile 2]
res4a_branch2a + res4a_branch2a_relu [profile 2]
res4a_branch2b + res4a_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res4a_branch2c_conv_residual_relu [profile 2]
res4b_branch2a + res4b_branch2a_relu [profile 2]
res4b_branch2b + res4b_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res4b_branch2c_conv_residual_relu [profile 2]
res4c_branch2a + res4c_branch2a_relu [profile 2]
res4c_branch2b + res4c_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res4c_branch2c_conv_residual_relu [profile 2]
res4d_branch2a + res4d_branch2a_relu [profile 2]
res4d_branch2b + res4d_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res4d_branch2c_conv_residual_relu [profile 2]
res4e_branch2a + res4e_branch2a_relu [profile 2]
res4e_branch2b + res4e_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res4e_branch2c_conv_residual_relu [profile 2]
res4f_branch2a + res4f_branch2a_relu [profile 2]
res4f_branch2b + res4f_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res4f_branch2c_conv_residual_relu [profile 2]
res5a_branch2a + res5a_branch2a_relu [profile 2]
res5a_branch1 [profile 2]
res5a_branch2b + res5a_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res5a_branch2c_conv_residual_relu [profile 2]
res5b_branch2a + res5b_branch2a_relu [profile 2]
res5b_branch2b + res5b_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res5b_branch2c_conv_residual_relu [profile 2]
res5c_branch2a + res5c_branch2a_relu [profile 2]
res5c_branch2b + res5c_branch2b_relu [profile 2]
SmallTileGEMM_TRT_res5c_branch2c_conv_residual_relu [profile 2]
squeeze_replaced [profile 2]
fc_replaced [profile 2]
topk_layer [profile 2]
cast_layer [profile 2]
conv1 + scale_conv1 + conv1_relu + pool1 [profile 3]
RES2_FULL_FUSION [profile 3]
res3a_branch1 [profile 3]
res3a_branch2a + res3a_branch2a_relu [profile 3]
res3a_branch2b + res3a_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res3a_branch2c_conv_residual_relu [profile 3]
res3b_branch2a + res3b_branch2a_relu [profile 3]
res3b_branch2b + res3b_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res3b_branch2c_conv_residual_relu [profile 3]
res3c_branch2a + res3c_branch2a_relu [profile 3]
res3c_branch2b + res3c_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res3c_branch2c_conv_residual_relu [profile 3]
res3d_branch2a + res3d_branch2a_relu [profile 3]
res3d_branch2b + res3d_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res3d_branch2c_conv_residual_relu [profile 3]
res4a_branch1 [profile 3]
res4a_branch2a + res4a_branch2a_relu [profile 3]
res4a_branch2b + res4a_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res4a_branch2c_conv_residual_relu [profile 3]
res4b_branch2a + res4b_branch2a_relu [profile 3]
res4b_branch2b + res4b_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res4b_branch2c_conv_residual_relu [profile 3]
res4c_branch2a + res4c_branch2a_relu [profile 3]
res4c_branch2b + res4c_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res4c_branch2c_conv_residual_relu [profile 3]
res4d_branch2a + res4d_branch2a_relu [profile 3]
res4d_branch2b + res4d_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res4d_branch2c_conv_residual_relu [profile 3]
res4e_branch2a + res4e_branch2a_relu [profile 3]
res4e_branch2b + res4e_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res4e_branch2c_conv_residual_relu [profile 3]
res4f_branch2a + res4f_branch2a_relu [profile 3]
res4f_branch2b + res4f_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res4f_branch2c_conv_residual_relu [profile 3]
res5a_branch2a + res5a_branch2a_relu [profile 3]
res5a_branch1 [profile 3]
res5a_branch2b + res5a_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res5a_branch2c_conv_residual_relu [profile 3]
res5b_branch2a + res5b_branch2a_relu [profile 3]
res5b_branch2b + res5b_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res5b_branch2c_conv_residual_relu [profile 3]
res5c_branch2a + res5c_branch2a_relu [profile 3]
res5c_branch2b + res5c_branch2b_relu [profile 3]
SmallTileGEMM_TRT_res5c_branch2c_conv_residual_relu [profile 3]
squeeze_replaced [profile 3]
fc_replaced [profile 3]
topk_layer [profile 3]
cast_layer [profile 3]

Bindings:
input_tensor_0
topk_layer_output_index
input_tensor_0 [profile 1]
topk_layer_output_index [profile 1]
input_tensor_0 [profile 2]
topk_layer_output_index [profile 2]
input_tensor_0 [profile 3]
topk_layer_output_index [profile 3]

[2023-12-15 18:53:39,809 generate_engines.py:176 INFO] Finished building engines for resnet50 benchmark in Server scenario.
Time taken to generate engines: 125.00753211975098 seconds
[2023-12-15 18:53:45,724 generate_engines.py:172 INFO] Building engines for rnnt benchmark in Offline scenario...
[2023-12-15 18:53:47,483 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:53:47] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 127, GPU 928 (MiB)
[12/15/2023-18:54:00] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4332, GPU +1150, now: CPU 4564, GPU 2078 (MiB)
[2023-12-15 18:54:00,548 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:54:00,549 rnn-t_builder.py:96 INFO] Loading RNN-T PyTorch model
[2023-12-15 18:54:00,792 rnn-t_builder.py:286 INFO] RNNTEncoderPlugin Plugin found
[2023-12-15 18:54:00,935 rnn-t_builder.py:286 INFO] RNNTEncoderPlugin Plugin found
[2023-12-15 18:54:01,224 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Offline/encoder.plan
[12/15/2023-18:54:01] [TRT] [I] Graph optimization time: 0.000481611 seconds.
[12/15/2023-18:54:01] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 4908, GPU 2146 (MiB)
[12/15/2023-18:54:01] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +2, GPU +70, now: CPU 4910, GPU 2216 (MiB)
[12/15/2023-18:54:01] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:54:04] [TRT] [I] Detected 7 inputs and 5 output network tensors.
[12/15/2023-18:54:04] [TRT] [I] Total Host Persistent Memory: 896
[12/15/2023-18:54:04] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:54:04] [TRT] [I] Total Scratch Memory: 3839901696
[12/15/2023-18:54:04] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 8 steps to complete.
[12/15/2023-18:54:04] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.045196ms to assign 5 blocks to 8 nodes requiring 4376781824 bytes.
[12/15/2023-18:54:04] [TRT] [I] Total Activation Memory: 4376780800
[12/15/2023-18:54:04] [TRT] [I] Total Weights Memory: 516
[12/15/2023-18:54:04] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +64, now: CPU 5052, GPU 2284 (MiB)
[12/15/2023-18:54:04] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +72, now: CPU 5052, GPU 2356 (MiB)
[12/15/2023-18:54:04] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 3072 MiB
[12/15/2023-18:54:04] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +1, now: CPU 0, GPU 1 (MiB)
[12/15/2023-18:54:04] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:54:04,979 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:54:04,979 builder.py:224 INFO] Layers:
Reformatting CopyNode for Network Input length
Reformatting CopyNode for Network Input length_host
(Unnamed Layer* 0) [Constant]
(Unnamed Layer* 1) [ElementWise]
(Unnamed Layer* 2) [Constant]
(Unnamed Layer* 3) [ElementWise]
encoder_pre_rnn
encoder_reshape
encoder_post_rnn

Bindings:
input
length
length_host
lower_hidden
upper_hidden
lower_cell
upper_cell
encoder_pre_rnn_hidden
encoder_pre_rnn_cell
encoder_post_rnn_output
encoder_post_rnn_hidden
encoder_post_rnn_cell

[2023-12-15 18:54:06,811 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:54:06] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 475, GPU 974 (MiB)
[12/15/2023-18:54:18] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4331, GPU +1128, now: CPU 4806, GPU 2102 (MiB)
[2023-12-15 18:54:18,798 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:54:18,800 rnn-t_builder.py:598 INFO] dec_embed_lut OUT tensor shape = (29, 320)
[2023-12-15 18:54:18,800 rnn-t_builder.py:599 INFO] dec_embedding OUT tensor shape = (-1, 1, 320)
[2023-12-15 18:54:18,800 rnn-t_builder.py:522 INFO] Decoder Plugin found
[2023-12-15 18:54:18,833 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Offline/decoder.plan
[12/15/2023-18:54:18] [TRT] [I] Graph optimization time: 0.0003193 seconds.
[12/15/2023-18:54:18] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +66, now: CPU 4813, GPU 2168 (MiB)
[12/15/2023-18:54:18] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4813, GPU 2232 (MiB)
[12/15/2023-18:54:18] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:54:19] [TRT] [I] Detected 3 inputs and 3 output network tensors.
[12/15/2023-18:54:19] [TRT] [I] Total Host Persistent Memory: 368
[12/15/2023-18:54:19] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:54:19] [TRT] [I] Total Scratch Memory: 18350080
[12/15/2023-18:54:19] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 2 steps to complete.
[12/15/2023-18:54:19] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.006464ms to assign 2 blocks to 2 nodes requiring 19660800 bytes.
[12/15/2023-18:54:19] [TRT] [I] Total Activation Memory: 19660800
[12/15/2023-18:54:19] [TRT] [I] Total Weights Memory: 18584
[12/15/2023-18:54:19] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 4821, GPU 2300 (MiB)
[12/15/2023-18:54:19] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4821, GPU 2364 (MiB)
[12/15/2023-18:54:19] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 3072 MiB
[12/15/2023-18:54:19] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +1, now: CPU 0, GPU 1 (MiB)
[12/15/2023-18:54:19] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:54:19,705 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:54:19,706 builder.py:224 INFO] Layers:
decoder_embedding
decoder_rnn

Bindings:
dec_embedding_input
hidden
cell
(Unnamed Layer* 2) [PluginV2DynamicExt]_output
(Unnamed Layer* 2) [PluginV2DynamicExt]_output_1
(Unnamed Layer* 2) [PluginV2DynamicExt]_output_2

[2023-12-15 18:54:21,349 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:54:21] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 489, GPU 982 (MiB)
[12/15/2023-18:54:32] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4332, GPU +1122, now: CPU 4821, GPU 2104 (MiB)
[2023-12-15 18:54:32,453 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:54:32,454 rnn-t_builder.py:991 INFO] Select Plugin found
[2023-12-15 18:54:32,454 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Offline/isel.plan
[12/15/2023-18:54:32] [TRT] [I] Graph optimization time: 0.000221147 seconds.
[12/15/2023-18:54:32] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +66, now: CPU 4821, GPU 2170 (MiB)
[12/15/2023-18:54:32] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4821, GPU 2234 (MiB)
[12/15/2023-18:54:32] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:54:32] [TRT] [I] Detected 7 inputs and 3 output network tensors.
[12/15/2023-18:54:32] [TRT] [I] Total Host Persistent Memory: 560
[12/15/2023-18:54:32] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:54:32] [TRT] [I] Total Scratch Memory: 0
[12/15/2023-18:54:32] [TRT] [I] Total Activation Memory: 0
[12/15/2023-18:54:32] [TRT] [I] Total Weights Memory: 0
[12/15/2023-18:54:32] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 4821, GPU 2298 (MiB)
[12/15/2023-18:54:32] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4821, GPU 2362 (MiB)
[12/15/2023-18:54:32] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 3072 MiB
[12/15/2023-18:54:32] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)
[12/15/2023-18:54:32] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:54:32,477 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:54:32,477 builder.py:224 INFO] Layers:
Select3

Bindings:
input0_hidden
input0_cell
input0_winner
input1_hidden
input1_cell
input1_winner
input_select
(Unnamed Layer* 0) [PluginV2DynamicExt]_output
(Unnamed Layer* 0) [PluginV2DynamicExt]_output_1
(Unnamed Layer* 0) [PluginV2DynamicExt]_output_2

[2023-12-15 18:54:34,047 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:54:34] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 489, GPU 986 (MiB)
[12/15/2023-18:54:44] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4332, GPU +1118, now: CPU 4821, GPU 2104 (MiB)
[2023-12-15 18:54:44,641 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:54:44,643 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Offline/igather.plan
[12/15/2023-18:54:44] [TRT] [I] Graph optimization time: 0.000202357 seconds.
[12/15/2023-18:54:44] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +66, now: CPU 4821, GPU 2170 (MiB)
[12/15/2023-18:54:44] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4821, GPU 2234 (MiB)
[12/15/2023-18:54:44] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:54:45] [TRT] [I] Detected 2 inputs and 1 output network tensors.
[12/15/2023-18:54:45] [TRT] [I] Total Host Persistent Memory: 32
[12/15/2023-18:54:45] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:54:45] [TRT] [I] Total Scratch Memory: 0
[12/15/2023-18:54:45] [TRT] [I] Total Activation Memory: 0
[12/15/2023-18:54:45] [TRT] [I] Total Weights Memory: 8
[12/15/2023-18:54:45] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 3072 MiB
[12/15/2023-18:54:45] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +1, now: CPU 0, GPU 1 (MiB)
[12/15/2023-18:54:45] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:54:45,071 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:54:45,071 builder.py:224 INFO] Layers:
Igather joint cell

Bindings:
encoder_input
t_coordinate
(Unnamed Layer* 0) [Gather]_output

[2023-12-15 18:54:46,632 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:54:46] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 489, GPU 988 (MiB)
[12/15/2023-18:54:57] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4332, GPU +1116, now: CPU 4821, GPU 2104 (MiB)
[2023-12-15 18:54:57,663 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:54:58,470 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Offline/fc1_a.plan
[12/15/2023-18:54:58] [TRT] [I] Graph optimization time: 0.00016338 seconds.
[12/15/2023-18:54:58] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +66, now: CPU 4823, GPU 2170 (MiB)
[12/15/2023-18:54:58] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4823, GPU 2234 (MiB)
[12/15/2023-18:54:58] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:55:03] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[12/15/2023-18:55:03] [TRT] [I] Total Host Persistent Memory: 9248
[12/15/2023-18:55:03] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:55:03] [TRT] [I] Total Scratch Memory: 0
[12/15/2023-18:55:03] [TRT] [I] Total Activation Memory: 0
[12/15/2023-18:55:03] [TRT] [I] Total Weights Memory: 1049600
[12/15/2023-18:55:03] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 3072 MiB
[12/15/2023-18:55:03] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +2, now: CPU 0, GPU 2 (MiB)
[12/15/2023-18:55:03] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:55:03,521 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:55:03,521 builder.py:224 INFO] Layers:
joint_fc1_a

Bindings:
enc_input
(Unnamed Layer* 0) [Fully Connected]_output

[2023-12-15 18:55:05,189 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:55:05] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 489, GPU 1026 (MiB)
[12/15/2023-18:55:16] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4332, GPU +1110, now: CPU 4821, GPU 2136 (MiB)
[2023-12-15 18:55:16,346 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:55:16,601 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Offline/fc1_b.plan
[12/15/2023-18:55:16] [TRT] [I] Graph optimization time: 0.000148133 seconds.
[12/15/2023-18:55:16] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +66, now: CPU 4822, GPU 2202 (MiB)
[12/15/2023-18:55:16] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4822, GPU 2266 (MiB)
[12/15/2023-18:55:16] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:55:19] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[12/15/2023-18:55:19] [TRT] [I] Total Host Persistent Memory: 7648
[12/15/2023-18:55:19] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:55:19] [TRT] [I] Total Scratch Memory: 0
[12/15/2023-18:55:19] [TRT] [I] Total Activation Memory: 0
[12/15/2023-18:55:19] [TRT] [I] Total Weights Memory: 327680
[12/15/2023-18:55:19] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 3072 MiB
[12/15/2023-18:55:19] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +1, now: CPU 0, GPU 1 (MiB)
[12/15/2023-18:55:19] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:55:19,982 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:55:19,982 builder.py:224 INFO] Layers:
joint_fc1_b

Bindings:
dec_input
(Unnamed Layer* 0) [Fully Connected]_output

[2023-12-15 18:55:21,632 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:55:21] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 499, GPU 1034 (MiB)
[12/15/2023-18:55:32] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4331, GPU +1102, now: CPU 4830, GPU 2136 (MiB)
[2023-12-15 18:55:32,632 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:55:32,635 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Offline/joint_backend.plan
[12/15/2023-18:55:32] [TRT] [I] Graph optimization time: 0.000349623 seconds.
[12/15/2023-18:55:32] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +66, now: CPU 4831, GPU 2202 (MiB)
[12/15/2023-18:55:32] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4831, GPU 2266 (MiB)
[12/15/2023-18:55:32] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:56:09] [TRT] [I] Detected 2 inputs and 1 output network tensors.
[12/15/2023-18:56:09] [TRT] [I] Total Host Persistent Memory: 8000
[12/15/2023-18:56:09] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:56:09] [TRT] [I] Total Scratch Memory: 65536
[12/15/2023-18:56:09] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 7 steps to complete.
[12/15/2023-18:56:09] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.024342ms to assign 3 blocks to 7 nodes requiring 2232320 bytes.
[12/15/2023-18:56:09] [TRT] [I] Total Activation Memory: 2232320
[12/15/2023-18:56:09] [TRT] [I] Total Weights Memory: 32832
[12/15/2023-18:56:09] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 3072 MiB
[12/15/2023-18:56:09] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +1, now: CPU 0, GPU 1 (MiB)
[12/15/2023-18:56:09] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:56:09,990 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:56:09,991 builder.py:224 INFO] Layers:
PWN(joint_fc1_sum + joint_relu)
joint_fc2_shuffle
Reformatting CopyNode for Input Tensor 0 to joint_fc2
joint_fc2
Reformatting CopyNode for Input Tensor 0 to joint_top1
joint_top1

Bindings:
joint_fc1_a_output
joint_fc1_b_output
(Unnamed Layer* 4) [TopK]_output_1

[2023-12-15 18:56:11,923 generate_engines.py:176 INFO] Finished building engines for rnnt benchmark in Offline scenario.
Loading TensorRT plugin from build/plugins/RNNTOptPlugin/librnntoptplugin.so
Initializing DALI with parameters:
	        __class__ : <class 'code.rnnt.dali.pipeline.DALIInferencePipeline'>
	 audio_fp16_input : True
	       batch_size : 16
	           device : gpu
	        device_id : 0
	           dither : 1e-05
	   frame_splicing : 3
	         highfreq : 0
	              log : True
	          lowfreq : 0
	     max_duration : 16.7
	            n_fft : 512
	            nfilt : 80
	        normalize : per_feature
	      num_threads : 2
	           pad_to : 8
	          preemph : 0.97
	processing_layout : tf
	   resample_range : None
	      sample_rate : 16000
	             self : <code.rnnt.dali.pipeline.DALIInferencePipeline object at 0x7f53b2df7b20>
	    total_samples : 16
	           window : hann
	      window_size : 0.02
	    window_stride : 0.01
self.n_fft = 512
self.hop_length = 160
self.win_length = 320
self.window_tensor = tensor([0.0000e+00, 9.6977e-05, 3.8791e-04, 8.7264e-04, 1.5510e-03, 2.4227e-03,
        3.4875e-03, 4.7449e-03, 6.1944e-03, 7.8355e-03, 9.6675e-03, 1.1690e-02,
        1.3901e-02, 1.6302e-02, 1.8890e-02, 2.1664e-02, 2.4624e-02, 2.7769e-02,
        3.1096e-02, 3.4606e-02, 3.8296e-02, 4.2165e-02, 4.6212e-02, 5.0435e-02,
        5.4833e-02, 5.9403e-02, 6.4144e-02, 6.9054e-02, 7.4131e-02, 7.9373e-02,
        8.4779e-02, 9.0346e-02, 9.6071e-02, 1.0195e-01, 1.0799e-01, 1.1418e-01,
        1.2052e-01, 1.2700e-01, 1.3363e-01, 1.4041e-01, 1.4732e-01, 1.5437e-01,
        1.6155e-01, 1.6886e-01, 1.7631e-01, 1.8388e-01, 1.9157e-01, 1.9938e-01,
        2.0730e-01, 2.1534e-01, 2.2350e-01, 2.3175e-01, 2.4012e-01, 2.4858e-01,
        2.5714e-01, 2.6580e-01, 2.7454e-01, 2.8338e-01, 2.9229e-01, 3.0129e-01,
        3.1037e-01, 3.1951e-01, 3.2873e-01, 3.3802e-01, 3.4737e-01, 3.5677e-01,
        3.6624e-01, 3.7575e-01, 3.8531e-01, 3.9492e-01, 4.0457e-01, 4.1425e-01,
        4.2397e-01, 4.3372e-01, 4.4349e-01, 4.5329e-01, 4.6310e-01, 4.7293e-01,
        4.8277e-01, 4.9261e-01, 5.0246e-01, 5.1231e-01, 5.2215e-01, 5.3198e-01,
        5.4181e-01, 5.5161e-01, 5.6140e-01, 5.7116e-01, 5.8089e-01, 5.9059e-01,
        6.0026e-01, 6.0989e-01, 6.1947e-01, 6.2901e-01, 6.3850e-01, 6.4794e-01,
        6.5732e-01, 6.6663e-01, 6.7588e-01, 6.8507e-01, 6.9418e-01, 7.0322e-01,
        7.1218e-01, 7.2105e-01, 7.2984e-01, 7.3854e-01, 7.4715e-01, 7.5566e-01,
        7.6408e-01, 7.7239e-01, 7.8059e-01, 7.8869e-01, 7.9667e-01, 8.0454e-01,
        8.1229e-01, 8.1992e-01, 8.2743e-01, 8.3481e-01, 8.4206e-01, 8.4917e-01,
        8.5616e-01, 8.6300e-01, 8.6970e-01, 8.7626e-01, 8.8267e-01, 8.8893e-01,
        8.9505e-01, 9.0101e-01, 9.0681e-01, 9.1246e-01, 9.1794e-01, 9.2327e-01,
        9.2843e-01, 9.3342e-01, 9.3825e-01, 9.4290e-01, 9.4739e-01, 9.5170e-01,
        9.5583e-01, 9.5979e-01, 9.6357e-01, 9.6717e-01, 9.7059e-01, 9.7383e-01,
        9.7688e-01, 9.7975e-01, 9.8243e-01, 9.8492e-01, 9.8723e-01, 9.8935e-01,
        9.9127e-01, 9.9301e-01, 9.9455e-01, 9.9591e-01, 9.9707e-01, 9.9804e-01,
        9.9881e-01, 9.9939e-01, 9.9978e-01, 9.9998e-01, 9.9998e-01, 9.9978e-01,
        9.9939e-01, 9.9881e-01, 9.9804e-01, 9.9707e-01, 9.9591e-01, 9.9455e-01,
        9.9301e-01, 9.9127e-01, 9.8935e-01, 9.8723e-01, 9.8492e-01, 9.8243e-01,
        9.7975e-01, 9.7688e-01, 9.7383e-01, 9.7059e-01, 9.6717e-01, 9.6357e-01,
        9.5979e-01, 9.5583e-01, 9.5170e-01, 9.4739e-01, 9.4290e-01, 9.3825e-01,
        9.3342e-01, 9.2843e-01, 9.2327e-01, 9.1794e-01, 9.1246e-01, 9.0681e-01,
        9.0101e-01, 8.9505e-01, 8.8893e-01, 8.8267e-01, 8.7626e-01, 8.6970e-01,
        8.6300e-01, 8.5616e-01, 8.4917e-01, 8.4206e-01, 8.3481e-01, 8.2743e-01,
        8.1992e-01, 8.1229e-01, 8.0454e-01, 7.9667e-01, 7.8869e-01, 7.8059e-01,
        7.7239e-01, 7.6408e-01, 7.5566e-01, 7.4715e-01, 7.3854e-01, 7.2984e-01,
        7.2105e-01, 7.1218e-01, 7.0322e-01, 6.9418e-01, 6.8507e-01, 6.7589e-01,
        6.6663e-01, 6.5732e-01, 6.4794e-01, 6.3850e-01, 6.2901e-01, 6.1947e-01,
        6.0989e-01, 6.0026e-01, 5.9059e-01, 5.8089e-01, 5.7116e-01, 5.6140e-01,
        5.5161e-01, 5.4181e-01, 5.3198e-01, 5.2215e-01, 5.1231e-01, 5.0246e-01,
        4.9261e-01, 4.8277e-01, 4.7293e-01, 4.6310e-01, 4.5329e-01, 4.4349e-01,
        4.3372e-01, 4.2397e-01, 4.1425e-01, 4.0457e-01, 3.9492e-01, 3.8531e-01,
        3.7575e-01, 3.6624e-01, 3.5677e-01, 3.4737e-01, 3.3802e-01, 3.2873e-01,
        3.1951e-01, 3.1037e-01, 3.0129e-01, 2.9229e-01, 2.8338e-01, 2.7454e-01,
        2.6580e-01, 2.5714e-01, 2.4858e-01, 2.4012e-01, 2.3175e-01, 2.2350e-01,
        2.1534e-01, 2.0730e-01, 1.9938e-01, 1.9157e-01, 1.8388e-01, 1.7631e-01,
        1.6886e-01, 1.6155e-01, 1.5437e-01, 1.4732e-01, 1.4041e-01, 1.3363e-01,
        1.2700e-01, 1.2052e-01, 1.1418e-01, 1.0799e-01, 1.0195e-01, 9.6071e-02,
        9.0346e-02, 8.4779e-02, 7.9373e-02, 7.4131e-02, 6.9054e-02, 6.4144e-02,
        5.9403e-02, 5.4833e-02, 5.0435e-02, 4.6212e-02, 4.2165e-02, 3.8296e-02,
        3.4606e-02, 3.1096e-02, 2.7769e-02, 2.4624e-02, 2.1664e-02, 1.8889e-02,
        1.6302e-02, 1.3901e-02, 1.1690e-02, 9.6675e-03, 7.8355e-03, 6.1944e-03,
        4.7449e-03, 3.4875e-03, 2.4227e-03, 1.5510e-03, 8.7264e-04, 3.8791e-04,
        9.6977e-05, 2.9802e-08])
self.sample_rate = 16000
self.window_size = 0.02
self.window_stride = 0.01
self.lowfreq = 0
self.device = gpu
Time taken to generate engines: 146.19816899299622 seconds
[2023-12-15 18:56:16,223 generate_engines.py:172 INFO] Building engines for rnnt benchmark in Server scenario...
[2023-12-15 18:56:17,981 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:56:17] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 127, GPU 928 (MiB)
[12/15/2023-18:56:30] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4332, GPU +1150, now: CPU 4564, GPU 2078 (MiB)
[2023-12-15 18:56:31,048 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:56:31,049 rnn-t_builder.py:96 INFO] Loading RNN-T PyTorch model
[2023-12-15 18:56:31,311 rnn-t_builder.py:286 INFO] RNNTEncoderPlugin Plugin found
[2023-12-15 18:56:31,456 rnn-t_builder.py:286 INFO] RNNTEncoderPlugin Plugin found
[2023-12-15 18:56:31,741 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Server/encoder.plan
[12/15/2023-18:56:31] [TRT] [I] Graph optimization time: 0.000492035 seconds.
[12/15/2023-18:56:31] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 4908, GPU 2146 (MiB)
[12/15/2023-18:56:31] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +2, GPU +70, now: CPU 4910, GPU 2216 (MiB)
[12/15/2023-18:56:31] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:56:34] [TRT] [I] Detected 7 inputs and 5 output network tensors.
[12/15/2023-18:56:35] [TRT] [I] Total Host Persistent Memory: 896
[12/15/2023-18:56:35] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:56:35] [TRT] [I] Total Scratch Memory: 3839901696
[12/15/2023-18:56:35] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 8 steps to complete.
[12/15/2023-18:56:35] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.042346ms to assign 5 blocks to 8 nodes requiring 4376781824 bytes.
[12/15/2023-18:56:35] [TRT] [I] Total Activation Memory: 4376780800
[12/15/2023-18:56:35] [TRT] [I] Total Weights Memory: 516
[12/15/2023-18:56:35] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +64, now: CPU 5052, GPU 2284 (MiB)
[12/15/2023-18:56:35] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +72, now: CPU 5052, GPU 2356 (MiB)
[12/15/2023-18:56:35] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 3072 MiB
[12/15/2023-18:56:35] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +1, now: CPU 0, GPU 1 (MiB)
[12/15/2023-18:56:35] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:56:35,516 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:56:35,516 builder.py:224 INFO] Layers:
Reformatting CopyNode for Network Input length
Reformatting CopyNode for Network Input length_host
(Unnamed Layer* 0) [Constant]
(Unnamed Layer* 1) [ElementWise]
(Unnamed Layer* 2) [Constant]
(Unnamed Layer* 3) [ElementWise]
encoder_pre_rnn
encoder_reshape
encoder_post_rnn

Bindings:
input
length
length_host
lower_hidden
upper_hidden
lower_cell
upper_cell
encoder_pre_rnn_hidden
encoder_pre_rnn_cell
encoder_post_rnn_output
encoder_post_rnn_hidden
encoder_post_rnn_cell

[2023-12-15 18:56:37,436 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:56:37] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 475, GPU 974 (MiB)
[12/15/2023-18:56:49] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4331, GPU +1128, now: CPU 4806, GPU 2102 (MiB)
[2023-12-15 18:56:49,678 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:56:49,679 rnn-t_builder.py:598 INFO] dec_embed_lut OUT tensor shape = (29, 320)
[2023-12-15 18:56:49,680 rnn-t_builder.py:599 INFO] dec_embedding OUT tensor shape = (-1, 1, 320)
[2023-12-15 18:56:49,680 rnn-t_builder.py:522 INFO] Decoder Plugin found
[2023-12-15 18:56:49,714 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Server/decoder.plan
[12/15/2023-18:56:49] [TRT] [I] Graph optimization time: 0.000332454 seconds.
[12/15/2023-18:56:49] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +66, now: CPU 4813, GPU 2168 (MiB)
[12/15/2023-18:56:49] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4813, GPU 2232 (MiB)
[12/15/2023-18:56:49] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:56:50] [TRT] [I] Detected 3 inputs and 3 output network tensors.
[12/15/2023-18:56:50] [TRT] [I] Total Host Persistent Memory: 368
[12/15/2023-18:56:50] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:56:50] [TRT] [I] Total Scratch Memory: 18350080
[12/15/2023-18:56:50] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 2 steps to complete.
[12/15/2023-18:56:50] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.007065ms to assign 2 blocks to 2 nodes requiring 19660800 bytes.
[12/15/2023-18:56:50] [TRT] [I] Total Activation Memory: 19660800
[12/15/2023-18:56:50] [TRT] [I] Total Weights Memory: 18584
[12/15/2023-18:56:50] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 4821, GPU 2300 (MiB)
[12/15/2023-18:56:50] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4821, GPU 2364 (MiB)
[12/15/2023-18:56:50] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 3072 MiB
[12/15/2023-18:56:50] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +1, now: CPU 0, GPU 1 (MiB)
[12/15/2023-18:56:50] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:56:50,642 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:56:50,643 builder.py:224 INFO] Layers:
decoder_embedding
decoder_rnn

Bindings:
dec_embedding_input
hidden
cell
(Unnamed Layer* 2) [PluginV2DynamicExt]_output
(Unnamed Layer* 2) [PluginV2DynamicExt]_output_1
(Unnamed Layer* 2) [PluginV2DynamicExt]_output_2

[2023-12-15 18:56:52,335 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:56:52] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 489, GPU 982 (MiB)
[12/15/2023-18:57:03] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4332, GPU +1122, now: CPU 4821, GPU 2104 (MiB)
[2023-12-15 18:57:03,672 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:57:03,675 rnn-t_builder.py:991 INFO] Select Plugin found
[2023-12-15 18:57:03,675 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Server/isel.plan
[12/15/2023-18:57:03] [TRT] [I] Graph optimization time: 0.000252067 seconds.
[12/15/2023-18:57:03] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +66, now: CPU 4821, GPU 2170 (MiB)
[12/15/2023-18:57:03] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4821, GPU 2234 (MiB)
[12/15/2023-18:57:03] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:57:03] [TRT] [I] Detected 7 inputs and 3 output network tensors.
[12/15/2023-18:57:03] [TRT] [I] Total Host Persistent Memory: 560
[12/15/2023-18:57:03] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:57:03] [TRT] [I] Total Scratch Memory: 0
[12/15/2023-18:57:03] [TRT] [I] Total Activation Memory: 0
[12/15/2023-18:57:03] [TRT] [I] Total Weights Memory: 0
[12/15/2023-18:57:03] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 4821, GPU 2298 (MiB)
[12/15/2023-18:57:03] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4821, GPU 2362 (MiB)
[12/15/2023-18:57:03] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 3072 MiB
[12/15/2023-18:57:03] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)
[12/15/2023-18:57:03] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:57:03,699 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:57:03,699 builder.py:224 INFO] Layers:
Select3

Bindings:
input0_hidden
input0_cell
input0_winner
input1_hidden
input1_cell
input1_winner
input_select
(Unnamed Layer* 0) [PluginV2DynamicExt]_output
(Unnamed Layer* 0) [PluginV2DynamicExt]_output_1
(Unnamed Layer* 0) [PluginV2DynamicExt]_output_2

[2023-12-15 18:57:05,359 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:57:05] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 489, GPU 986 (MiB)
[12/15/2023-18:57:17] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4332, GPU +1118, now: CPU 4821, GPU 2104 (MiB)
[2023-12-15 18:57:17,131 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:57:17,132 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Server/igather.plan
[12/15/2023-18:57:17] [TRT] [I] Graph optimization time: 0.000205693 seconds.
[12/15/2023-18:57:17] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +66, now: CPU 4821, GPU 2170 (MiB)
[12/15/2023-18:57:17] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4821, GPU 2234 (MiB)
[12/15/2023-18:57:17] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:57:17] [TRT] [I] Detected 2 inputs and 1 output network tensors.
[12/15/2023-18:57:17] [TRT] [I] Total Host Persistent Memory: 32
[12/15/2023-18:57:17] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:57:17] [TRT] [I] Total Scratch Memory: 0
[12/15/2023-18:57:17] [TRT] [I] Total Activation Memory: 0
[12/15/2023-18:57:17] [TRT] [I] Total Weights Memory: 8
[12/15/2023-18:57:17] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 3072 MiB
[12/15/2023-18:57:17] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +1, now: CPU 0, GPU 1 (MiB)
[12/15/2023-18:57:17] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:57:17,593 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:57:17,593 builder.py:224 INFO] Layers:
Igather joint cell

Bindings:
encoder_input
t_coordinate
(Unnamed Layer* 0) [Gather]_output

[2023-12-15 18:57:19,279 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:57:19] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 489, GPU 988 (MiB)
[12/15/2023-18:57:31] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4332, GPU +1116, now: CPU 4821, GPU 2104 (MiB)
[2023-12-15 18:57:31,183 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:57:32,097 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Server/fc1_a.plan
[12/15/2023-18:57:32] [TRT] [I] Graph optimization time: 0.000163392 seconds.
[12/15/2023-18:57:32] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +66, now: CPU 4823, GPU 2170 (MiB)
[12/15/2023-18:57:32] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4823, GPU 2234 (MiB)
[12/15/2023-18:57:32] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:57:37] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[12/15/2023-18:57:37] [TRT] [I] Total Host Persistent Memory: 9248
[12/15/2023-18:57:37] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:57:37] [TRT] [I] Total Scratch Memory: 0
[12/15/2023-18:57:37] [TRT] [I] Total Activation Memory: 0
[12/15/2023-18:57:37] [TRT] [I] Total Weights Memory: 1049600
[12/15/2023-18:57:37] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 3072 MiB
[12/15/2023-18:57:37] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +2, now: CPU 0, GPU 2 (MiB)
[12/15/2023-18:57:37] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:57:37,125 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:57:37,125 builder.py:224 INFO] Layers:
joint_fc1_a

Bindings:
enc_input
(Unnamed Layer* 0) [Fully Connected]_output

[2023-12-15 18:57:38,792 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:57:38] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 489, GPU 1026 (MiB)
[12/15/2023-18:57:50] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4332, GPU +1110, now: CPU 4821, GPU 2136 (MiB)
[2023-12-15 18:57:50,104 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:57:50,386 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Server/fc1_b.plan
[12/15/2023-18:57:50] [TRT] [I] Graph optimization time: 0.000158739 seconds.
[12/15/2023-18:57:50] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +66, now: CPU 4822, GPU 2202 (MiB)
[12/15/2023-18:57:50] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4822, GPU 2266 (MiB)
[12/15/2023-18:57:50] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:57:53] [TRT] [I] Detected 1 inputs and 1 output network tensors.
[12/15/2023-18:57:53] [TRT] [I] Total Host Persistent Memory: 7648
[12/15/2023-18:57:53] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:57:53] [TRT] [I] Total Scratch Memory: 0
[12/15/2023-18:57:53] [TRT] [I] Total Activation Memory: 0
[12/15/2023-18:57:53] [TRT] [I] Total Weights Memory: 327680
[12/15/2023-18:57:53] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 3072 MiB
[12/15/2023-18:57:53] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +1, now: CPU 0, GPU 1 (MiB)
[12/15/2023-18:57:53] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:57:53,828 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:57:53,828 builder.py:224 INFO] Layers:
joint_fc1_b

Bindings:
dec_input
(Unnamed Layer* 0) [Fully Connected]_output

[2023-12-15 18:57:55,475 rnn-t_builder.py:102 INFO] Using workspace size: 4294967296
[12/15/2023-18:57:55] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 499, GPU 1034 (MiB)
[12/15/2023-18:58:06] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4331, GPU +1102, now: CPU 4830, GPU 2136 (MiB)
[2023-12-15 18:58:06,576 rnn-t_builder.py:114 INFO] Enabling cuDNN/cuBLAS/cuBLASLt tactics for TRT 8.6 on datacenter systems
[2023-12-15 18:58:06,578 builder.py:182 INFO] Building ./build/engines/ACC_H100/rnnt/Server/joint_backend.plan
[12/15/2023-18:58:06] [TRT] [I] Graph optimization time: 0.000373361 seconds.
[12/15/2023-18:58:06] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +66, now: CPU 4831, GPU 2202 (MiB)
[12/15/2023-18:58:06] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 4831, GPU 2266 (MiB)
[12/15/2023-18:58:06] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.
[12/15/2023-18:58:43] [TRT] [I] Detected 2 inputs and 1 output network tensors.
[12/15/2023-18:58:44] [TRT] [I] Total Host Persistent Memory: 8000
[12/15/2023-18:58:44] [TRT] [I] Total Device Persistent Memory: 0
[12/15/2023-18:58:44] [TRT] [I] Total Scratch Memory: 65536
[12/15/2023-18:58:44] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 7 steps to complete.
[12/15/2023-18:58:44] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.030125ms to assign 3 blocks to 7 nodes requiring 2232320 bytes.
[12/15/2023-18:58:44] [TRT] [I] Total Activation Memory: 2232320
[12/15/2023-18:58:44] [TRT] [I] Total Weights Memory: 32832
[12/15/2023-18:58:44] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1 MiB, GPU 3072 MiB
[12/15/2023-18:58:44] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +1, now: CPU 0, GPU 1 (MiB)
[12/15/2023-18:58:44] [TRT] [I] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[2023-12-15 18:58:44,141 builder.py:223 INFO] ========= TensorRT Engine Layer Information =========
[2023-12-15 18:58:44,142 builder.py:224 INFO] Layers:
PWN(joint_fc1_sum + joint_relu)
joint_fc2_shuffle
Reformatting CopyNode for Input Tensor 0 to joint_fc2
joint_fc2
Reformatting CopyNode for Input Tensor 0 to joint_top1
joint_top1

Bindings:
joint_fc1_a_output
joint_fc1_b_output
(Unnamed Layer* 4) [TopK]_output_1

[2023-12-15 18:58:46,213 generate_engines.py:176 INFO] Finished building engines for rnnt benchmark in Server scenario.
Loading TensorRT plugin from build/plugins/RNNTOptPlugin/librnntoptplugin.so
Initializing DALI with parameters:
	        __class__ : <class 'code.rnnt.dali.pipeline.DALIInferencePipeline'>
	 audio_fp16_input : True
	       batch_size : 16
	           device : gpu
	        device_id : 0
	           dither : 1e-05
	   frame_splicing : 3
	         highfreq : 0
	              log : True
	          lowfreq : 0
	     max_duration : 16.7
	            n_fft : 512
	            nfilt : 80
	        normalize : per_feature
	      num_threads : 2
	           pad_to : 8
	          preemph : 0.97
	processing_layout : tf
	   resample_range : None
	      sample_rate : 16000
	             self : <code.rnnt.dali.pipeline.DALIInferencePipeline object at 0x7f50f88a9b20>
	    total_samples : 16
	           window : hann
	      window_size : 0.02
	    window_stride : 0.01
self.n_fft = 512
self.hop_length = 160
self.win_length = 320
self.window_tensor = tensor([0.0000e+00, 9.6977e-05, 3.8791e-04, 8.7264e-04, 1.5510e-03, 2.4227e-03,
        3.4875e-03, 4.7449e-03, 6.1944e-03, 7.8355e-03, 9.6675e-03, 1.1690e-02,
        1.3901e-02, 1.6302e-02, 1.8890e-02, 2.1664e-02, 2.4624e-02, 2.7769e-02,
        3.1096e-02, 3.4606e-02, 3.8296e-02, 4.2165e-02, 4.6212e-02, 5.0435e-02,
        5.4833e-02, 5.9403e-02, 6.4144e-02, 6.9054e-02, 7.4131e-02, 7.9373e-02,
        8.4779e-02, 9.0346e-02, 9.6071e-02, 1.0195e-01, 1.0799e-01, 1.1418e-01,
        1.2052e-01, 1.2700e-01, 1.3363e-01, 1.4041e-01, 1.4732e-01, 1.5437e-01,
        1.6155e-01, 1.6886e-01, 1.7631e-01, 1.8388e-01, 1.9157e-01, 1.9938e-01,
        2.0730e-01, 2.1534e-01, 2.2350e-01, 2.3175e-01, 2.4012e-01, 2.4858e-01,
        2.5714e-01, 2.6580e-01, 2.7454e-01, 2.8338e-01, 2.9229e-01, 3.0129e-01,
        3.1037e-01, 3.1951e-01, 3.2873e-01, 3.3802e-01, 3.4737e-01, 3.5677e-01,
        3.6624e-01, 3.7575e-01, 3.8531e-01, 3.9492e-01, 4.0457e-01, 4.1425e-01,
        4.2397e-01, 4.3372e-01, 4.4349e-01, 4.5329e-01, 4.6310e-01, 4.7293e-01,
        4.8277e-01, 4.9261e-01, 5.0246e-01, 5.1231e-01, 5.2215e-01, 5.3198e-01,
        5.4181e-01, 5.5161e-01, 5.6140e-01, 5.7116e-01, 5.8089e-01, 5.9059e-01,
        6.0026e-01, 6.0989e-01, 6.1947e-01, 6.2901e-01, 6.3850e-01, 6.4794e-01,
        6.5732e-01, 6.6663e-01, 6.7588e-01, 6.8507e-01, 6.9418e-01, 7.0322e-01,
        7.1218e-01, 7.2105e-01, 7.2984e-01, 7.3854e-01, 7.4715e-01, 7.5566e-01,
        7.6408e-01, 7.7239e-01, 7.8059e-01, 7.8869e-01, 7.9667e-01, 8.0454e-01,
        8.1229e-01, 8.1992e-01, 8.2743e-01, 8.3481e-01, 8.4206e-01, 8.4917e-01,
        8.5616e-01, 8.6300e-01, 8.6970e-01, 8.7626e-01, 8.8267e-01, 8.8893e-01,
        8.9505e-01, 9.0101e-01, 9.0681e-01, 9.1246e-01, 9.1794e-01, 9.2327e-01,
        9.2843e-01, 9.3342e-01, 9.3825e-01, 9.4290e-01, 9.4739e-01, 9.5170e-01,
        9.5583e-01, 9.5979e-01, 9.6357e-01, 9.6717e-01, 9.7059e-01, 9.7383e-01,
        9.7688e-01, 9.7975e-01, 9.8243e-01, 9.8492e-01, 9.8723e-01, 9.8935e-01,
        9.9127e-01, 9.9301e-01, 9.9455e-01, 9.9591e-01, 9.9707e-01, 9.9804e-01,
        9.9881e-01, 9.9939e-01, 9.9978e-01, 9.9998e-01, 9.9998e-01, 9.9978e-01,
        9.9939e-01, 9.9881e-01, 9.9804e-01, 9.9707e-01, 9.9591e-01, 9.9455e-01,
        9.9301e-01, 9.9127e-01, 9.8935e-01, 9.8723e-01, 9.8492e-01, 9.8243e-01,
        9.7975e-01, 9.7688e-01, 9.7383e-01, 9.7059e-01, 9.6717e-01, 9.6357e-01,
        9.5979e-01, 9.5583e-01, 9.5170e-01, 9.4739e-01, 9.4290e-01, 9.3825e-01,
        9.3342e-01, 9.2843e-01, 9.2327e-01, 9.1794e-01, 9.1246e-01, 9.0681e-01,
        9.0101e-01, 8.9505e-01, 8.8893e-01, 8.8267e-01, 8.7626e-01, 8.6970e-01,
        8.6300e-01, 8.5616e-01, 8.4917e-01, 8.4206e-01, 8.3481e-01, 8.2743e-01,
        8.1992e-01, 8.1229e-01, 8.0454e-01, 7.9667e-01, 7.8869e-01, 7.8059e-01,
        7.7239e-01, 7.6408e-01, 7.5566e-01, 7.4715e-01, 7.3854e-01, 7.2984e-01,
        7.2105e-01, 7.1218e-01, 7.0322e-01, 6.9418e-01, 6.8507e-01, 6.7589e-01,
        6.6663e-01, 6.5732e-01, 6.4794e-01, 6.3850e-01, 6.2901e-01, 6.1947e-01,
        6.0989e-01, 6.0026e-01, 5.9059e-01, 5.8089e-01, 5.7116e-01, 5.6140e-01,
        5.5161e-01, 5.4181e-01, 5.3198e-01, 5.2215e-01, 5.1231e-01, 5.0246e-01,
        4.9261e-01, 4.8277e-01, 4.7293e-01, 4.6310e-01, 4.5329e-01, 4.4349e-01,
        4.3372e-01, 4.2397e-01, 4.1425e-01, 4.0457e-01, 3.9492e-01, 3.8531e-01,
        3.7575e-01, 3.6624e-01, 3.5677e-01, 3.4737e-01, 3.3802e-01, 3.2873e-01,
        3.1951e-01, 3.1037e-01, 3.0129e-01, 2.9229e-01, 2.8338e-01, 2.7454e-01,
        2.6580e-01, 2.5714e-01, 2.4858e-01, 2.4012e-01, 2.3175e-01, 2.2350e-01,
        2.1534e-01, 2.0730e-01, 1.9938e-01, 1.9157e-01, 1.8388e-01, 1.7631e-01,
        1.6886e-01, 1.6155e-01, 1.5437e-01, 1.4732e-01, 1.4041e-01, 1.3363e-01,
        1.2700e-01, 1.2052e-01, 1.1418e-01, 1.0799e-01, 1.0195e-01, 9.6071e-02,
        9.0346e-02, 8.4779e-02, 7.9373e-02, 7.4131e-02, 6.9054e-02, 6.4144e-02,
        5.9403e-02, 5.4833e-02, 5.0435e-02, 4.6212e-02, 4.2165e-02, 3.8296e-02,
        3.4606e-02, 3.1096e-02, 2.7769e-02, 2.4624e-02, 2.1664e-02, 1.8889e-02,
        1.6302e-02, 1.3901e-02, 1.1690e-02, 9.6675e-03, 7.8355e-03, 6.1944e-03,
        4.7449e-03, 3.4875e-03, 2.4227e-03, 1.5510e-03, 8.7264e-04, 3.8791e-04,
        9.6977e-05, 2.9802e-08])
self.sample_rate = 16000
self.window_size = 0.02
self.window_stride = 0.01
self.lowfreq = 0
self.device = gpu
Time taken to generate engines: 149.98944282531738 seconds
make[1]: Leaving directory '/work'
make[1]: Entering directory '/work'
[2023-12-15 19:00:03,246 main.py:230 INFO] Detected system ID: KnownSystem.ACC_H100
[2023-12-15 19:00:06,235 generate_conf_files.py:107 INFO] Generated measurements/ entries for ACC_H100_TRT/bert-99/Offline
[2023-12-15 19:00:06,235 __init__.py:46 INFO] Running command: ./build/bin/harness_bert --logfile_outdir="/work/build/logs/2023.12.15-17.40.48/ACC_H100_TRT/bert-99/Offline" --logfile_prefix="mlperf_log_" --performance_sample_count=10833 --gpu_batch_size=1280 --tensor_path="build/preprocessed_data/squad_tokenized/input_ids.npy,build/preprocessed_data/squad_tokenized/segment_ids.npy,build/preprocessed_data/squad_tokenized/input_mask.npy" --use_graphs=false --gpu_inference_streams=2 --gpu_copy_streams=2 --gpu_engines="./build/engines/ACC_H100/bert/Offline/bert-Offline-gpu-int8_S_384_B_1280_P_2_vs.custom_k_99_MaxP.plan" --mlperf_conf_path="build/loadgen-configs/ACC_H100_TRT/bert-99/Offline/mlperf.conf" --user_conf_path="build/loadgen-configs/ACC_H100_TRT/bert-99/Offline/user.conf" --scenario Offline --model bert
[2023-12-15 19:00:06,235 __init__.py:53 INFO] Overriding Environment
benchmark : Benchmark.BERT
buffer_manager_thread_count : 0
coalesced_tensor : True
data_dir : /home/zhouf/mlperf_inference_data/data
enable_interleaved : False
gpu_batch_size : 1280
gpu_copy_streams : 2
gpu_inference_streams : 2
input_dtype : int32
input_format : linear
log_dir : /work/build/logs/2023.12.15-17.40.48
offline_expected_qps : 5700
precision : int8
preprocessed_data_dir : /home/zhouf/mlperf_inference_data/preprocessed_data
scenario : Scenario.Offline
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='GENUINE INTEL(R) XEON(R)', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=72, threads_per_core=2): 2}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=395.271148, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=395271148000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA H100 PCIe', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=79.6474609375, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=85520809984), max_power_limit=350.0, pci_id='0x233110DE', compute_sm=90): 1})), numa_conf=None, system_id='ACC_H100')
tensor_path : build/preprocessed_data/squad_tokenized/input_ids.npy,build/preprocessed_data/squad_tokenized/segment_ids.npy,build/preprocessed_data/squad_tokenized/input_mask.npy
use_graphs : False
use_small_tile_gemm_plugin : False
system_id : ACC_H100
config_name : ACC_H100_bert_Offline
workload_setting : WorkloadSetting(HarnessType.Custom, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
use_cpu : False
use_inferentia : False
num_profiles : 2
config_ver : custom_k_99_MaxP
accuracy_level : 99%
inference_server : custom
skip_file_checks : True
power_limit : None
cpu_freq : None
&&&& RUNNING BERT_HARNESS # ./build/bin/harness_bert
I1215 19:00:06.325702 39926 main_bert.cc:163] Found 1 GPUs
I1215 19:00:07.467907 39926 bert_server.cc:142] Engine Path: ./build/engines/ACC_H100/bert/Offline/bert-Offline-gpu-int8_S_384_B_1280_P_2_vs.custom_k_99_MaxP.plan
[I] [TRT] Loaded engine size: 414 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +7, GPU +64, now: CPU 588, GPU 1414 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 590, GPU 1486 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +290, now: CPU 0, GPU 290 (MiB)
I1215 19:00:08.047999 39926 bert_server.cc:203] Engines Creation Completed
I1215 19:00:08.055301 39926 bert_core_vs.cc:385] Engine - Device Memory requirements: 3523217408
I1215 19:00:08.055316 39926 bert_core_vs.cc:393] Engine - Number of Optimization Profiles: 2
I1215 19:00:08.055330 39926 bert_core_vs.cc:415] Engine - Profile 0 maxDims 491520 Bmax=1280 Smax=384
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 176, GPU 4784 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 176, GPU 4848 (MiB)
I1215 19:00:08.166458 39926 bert_core_vs.cc:426] Setting Opt.Prof. to 0
I1215 19:00:08.166507 39926 bert_core_vs.cc:444] Context creation complete. Max supported batchSize: 1280
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +0, now: CPU 1, GPU 290 (MiB)
I1215 19:00:08.172897 39926 bert_core_vs.cc:476] Setup complete
I1215 19:00:08.176826 39926 bert_core_vs.cc:385] Engine - Device Memory requirements: 3523217408
I1215 19:00:08.176834 39926 bert_core_vs.cc:393] Engine - Number of Optimization Profiles: 2
I1215 19:00:08.176841 39926 bert_core_vs.cc:415] Engine - Profile 1 maxDims 491520 Bmax=1280 Smax=384
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 299, GPU 8410 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +74, now: CPU 300, GPU 8484 (MiB)
I1215 19:00:08.278239 39926 bert_core_vs.cc:426] Setting Opt.Prof. to 1
[I] [TRT] Could not set default profile 0 for execution context. Profile index must be set explicitly.
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +0, now: CPU 2, GPU 290 (MiB)
I1215 19:00:08.280324 39926 bert_core_vs.cc:444] Context creation complete. Max supported batchSize: 1280
I1215 19:00:08.287842 39926 bert_core_vs.cc:476] Setup complete
I1215 19:00:09.001638 39926 main_bert.cc:184] Starting running actual test.
I1215 19:11:54.206068 39926 main_bert.cc:190] Finished running actual test.
================================================
MLPerf Results Summary
================================================
SUT name : BERT SERVER
Scenario : Offline
Mode     : PerformanceOnly
Samples per second: 5343.95
Result is : VALID
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: Yes

================================================
Additional Stats
================================================
Min latency (ns)                : 3547214040
Max latency (ns)                : 703973428703
Mean latency (ns)               : 425837722582
50.00 percentile latency (ns)   : 453788423961
90.00 percentile latency (ns)   : 670809653815
95.00 percentile latency (ns)   : 690239237458
97.00 percentile latency (ns)   : 696750829367
99.00 percentile latency (ns)   : 701956214783
99.90 percentile latency (ns)   : 703908789531

================================================
Test Parameters Used
================================================
samples_per_query : 3762000
target_qps : 5700
target_latency (ns): 0
max_async_queries : 1
min_duration (ms): 600000
max_duration (ms): 0
min_query_count : 1
max_query_count : 0
qsl_rng_seed : 148687905518835231
sample_index_rng_seed : 520418551913322573
schedule_rng_seed : 811580660758947900
accuracy_log_rng_seed : 0
accuracy_log_probability : 0
accuracy_log_sampling_target : 0
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 10833

2 warnings encountered. See detailed log.

No errors encountered during test.
[2023-12-15 19:11:55,300 run_harness.py:167 INFO] Result: result_samples_per_second: 5343.95, Result is VALID
[2023-12-15 19:11:55,312 generate_conf_files.py:107 INFO] Generated measurements/ entries for ACC_H100_TRT/bert-99/Server
[2023-12-15 19:11:55,313 __init__.py:46 INFO] Running command: ./build/bin/harness_bert --logfile_outdir="/work/build/logs/2023.12.15-17.40.48/ACC_H100_TRT/bert-99/Server" --logfile_prefix="mlperf_log_" --performance_sample_count=10833 --server_num_issue_query_threads=1 --gpu_batch_size=256 --tensor_path="build/preprocessed_data/squad_tokenized/input_ids.npy,build/preprocessed_data/squad_tokenized/segment_ids.npy,build/preprocessed_data/squad_tokenized/input_mask.npy" --use_graphs=false --gpu_inference_streams=2 --gpu_copy_streams=1 --soft_drop=0.99 --gpu_engines="./build/engines/ACC_H100/bert/Server/bert-Server-gpu-int8_S_384_B_256_P_2_vs.custom_k_99_MaxP.plan" --mlperf_conf_path="build/loadgen-configs/ACC_H100_TRT/bert-99/Server/mlperf.conf" --user_conf_path="build/loadgen-configs/ACC_H100_TRT/bert-99/Server/user.conf" --scenario Server --model bert
[2023-12-15 19:11:55,313 __init__.py:53 INFO] Overriding Environment
benchmark : Benchmark.BERT
buffer_manager_thread_count : 0
coalesced_tensor : True
data_dir : /home/zhouf/mlperf_inference_data/data
enable_interleaved : False
gpu_batch_size : 256
gpu_copy_streams : 1
gpu_inference_streams : 2
input_dtype : int32
input_format : linear
log_dir : /work/build/logs/2023.12.15-17.40.48
precision : int8
preprocessed_data_dir : /home/zhouf/mlperf_inference_data/preprocessed_data
scenario : Scenario.Server
server_num_issue_query_threads : 1
server_target_qps : 4560
soft_drop : 0.99
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='GENUINE INTEL(R) XEON(R)', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=72, threads_per_core=2): 2}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=395.271148, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=395271148000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA H100 PCIe', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=79.6474609375, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=85520809984), max_power_limit=350.0, pci_id='0x233110DE', compute_sm=90): 1})), numa_conf=None, system_id='ACC_H100')
tensor_path : build/preprocessed_data/squad_tokenized/input_ids.npy,build/preprocessed_data/squad_tokenized/segment_ids.npy,build/preprocessed_data/squad_tokenized/input_mask.npy
use_graphs : False
use_small_tile_gemm_plugin : False
system_id : ACC_H100
config_name : ACC_H100_bert_Server
workload_setting : WorkloadSetting(HarnessType.Custom, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
use_cpu : False
use_inferentia : False
num_profiles : 1
config_ver : custom_k_99_MaxP
accuracy_level : 99%
inference_server : custom
skip_file_checks : True
power_limit : None
cpu_freq : None
&&&& RUNNING BERT_HARNESS # ./build/bin/harness_bert
I1215 19:11:55.414813 39936 main_bert.cc:163] Found 1 GPUs
I1215 19:11:56.562269 39936 bert_server.cc:142] Engine Path: ./build/engines/ACC_H100/bert/Server/bert-Server-gpu-int8_S_384_B_256_P_2_vs.custom_k_99_MaxP.plan
[I] [TRT] Loaded engine size: 414 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +64, now: CPU 588, GPU 1414 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 590, GPU 1486 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +290, now: CPU 0, GPU 290 (MiB)
I1215 19:11:57.150607 39936 bert_server.cc:203] Engines Creation Completed
I1215 19:11:57.154798 39936 bert_core_vs.cc:385] Engine - Device Memory requirements: 704645120
I1215 19:11:57.154814 39936 bert_core_vs.cc:393] Engine - Number of Optimization Profiles: 2
I1215 19:11:57.154825 39936 bert_core_vs.cc:415] Engine - Profile 0 maxDims 98304 Bmax=256 Smax=384
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 176, GPU 2096 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +64, now: CPU 177, GPU 2160 (MiB)
I1215 19:11:57.264097 39936 bert_core_vs.cc:426] Setting Opt.Prof. to 0
I1215 19:11:57.264138 39936 bert_core_vs.cc:444] Context creation complete. Max supported batchSize: 256
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +0, now: CPU 1, GPU 290 (MiB)
I1215 19:11:57.264430 39936 bert_core_vs.cc:476] Setup complete
I1215 19:11:57.265223 39936 bert_core_vs.cc:385] Engine - Device Memory requirements: 704645120
I1215 19:11:57.265226 39936 bert_core_vs.cc:393] Engine - Number of Optimization Profiles: 2
I1215 19:11:57.265233 39936 bert_core_vs.cc:415] Engine - Profile 1 maxDims 98304 Bmax=256 Smax=384
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 300, GPU 3024 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +72, now: CPU 300, GPU 3096 (MiB)
I1215 19:11:57.368674 39936 bert_core_vs.cc:426] Setting Opt.Prof. to 1
[I] [TRT] Could not set default profile 0 for execution context. Profile index must be set explicitly.
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +0, now: CPU 2, GPU 290 (MiB)
I1215 19:11:57.370915 39936 bert_core_vs.cc:444] Context creation complete. Max supported batchSize: 256
I1215 19:11:57.372282 39936 bert_core_vs.cc:476] Setup complete
I1215 19:11:57.372294 39936 bert_server.cc:244] Apply soft drop policy with threshold = 0.99
I1215 19:11:57.372330 39936 bert_server.cc:250] Use number of server IssueQuery threads = 1
I1215 19:11:57.526224 39936 main_bert.cc:184] Starting running actual test.
I1215 19:22:31.620169 39944 bert_server.cc:88] Total number of soft drop tasks: 13693 out of 1368090 total tasks
I1215 19:22:31.734948 39943 bert_server.cc:88] Total number of soft drop tasks: 13637 out of 1370375 total tasks
I1215 19:22:40.964146 39936 main_bert.cc:190] Finished running actual test.
================================================
MLPerf Results Summary
================================================
SUT name : BERT SERVER
Scenario : Server
Mode     : PerformanceOnly
Scheduled samples per second : 4564.11
Result is : INVALID
  Performance constraints satisfied : NO
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: NO
Recommendations:
 * Reduce target QPS to improve latency.
Early Stopping Result:
 * Run unsuccessful.
 * Processed 2738465 queries.
 * Would need to run at least 269906914 more queries,
 with the run being successful if every additional
 query were under latency.

================================================
Additional Stats
================================================
Completed samples per second    : 4345.31

Min latency (ns)                : 5548415
Max latency (ns)                : 636375282572
Mean latency (ns)               : 17538569750
50.00 percentile latency (ns)   : 14552014602
90.00 percentile latency (ns)   : 27653779976
95.00 percentile latency (ns)   : 28931858659
97.00 percentile latency (ns)   : 29535788933
99.00 percentile latency (ns)   : 30259692453
99.90 percentile latency (ns)   : 575848712179

================================================
Test Parameters Used
================================================
samples_per_query : 1
target_qps : 4560
target_latency (ns): 130000000
max_async_queries : 0
min_duration (ms): 600000
max_duration (ms): 0
min_query_count : 100
max_query_count : 0
qsl_rng_seed : 148687905518835231
sample_index_rng_seed : 520418551913322573
schedule_rng_seed : 811580660758947900
accuracy_log_rng_seed : 0
accuracy_log_probability : 0
accuracy_log_sampling_target : 0
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 10833

No warnings encountered during test.

No errors encountered during test.
[2023-12-15 19:22:42,849 run_harness.py:167 INFO] Result: result_scheduled_samples_per_sec: 4564.11, Result is INVALID
[2023-12-15 19:22:42,863 harness.py:236 INFO] The harness will load 1 plugins: ['build/plugins/../TRTLLM/cpp/build/tensorrt_llm/plugins/libnvinfer_plugin.so']
[2023-12-15 19:22:42,863 generate_conf_files.py:107 INFO] Generated measurements/ entries for ACC_H100_TRT/gptj-99/Offline
[2023-12-15 19:22:42,864 __init__.py:46 INFO] Running command: ./build/bin/harness_gpt --plugins="build/plugins/../TRTLLM/cpp/build/tensorrt_llm/plugins/libnvinfer_plugin.so" --logfile_outdir="/work/build/logs/2023.12.15-17.40.48/ACC_H100_TRT/gptj-99/Offline" --logfile_prefix="mlperf_log_" --performance_sample_count=13368 --gpu_batch_size=32 --tensor_path="build/preprocessed_data/cnn_dailymail_tokenized_gptj/input_ids_padded.npy,build/preprocessed_data/cnn_dailymail_tokenized_gptj/masked_tokens.npy,build/preprocessed_data/cnn_dailymail_tokenized_gptj/input_lengths.npy" --use_graphs=false --use_fp8=true --gpu_inference_streams=1 --gpu_copy_streams=1 --tensor_parallelism=1 --enable_sort=true --num_sort_segments=2 --gpu_engines="./build/engines/ACC_H100/gptj/Offline/gptj-Offline-gpu-b32-fp16.custom_k_99_MaxP.plan" --mlperf_conf_path="build/loadgen-configs/ACC_H100_TRT/gptj-99/Offline/mlperf.conf" --user_conf_path="build/loadgen-configs/ACC_H100_TRT/gptj-99/Offline/user.conf" --scenario Offline --model gptj
[2023-12-15 19:22:42,864 __init__.py:53 INFO] Overriding Environment
benchmark : Benchmark.GPTJ
buffer_manager_thread_count : 0
coalesced_tensor : True
data_dir : /home/zhouf/mlperf_inference_data/data
enable_sort : True
gpu_batch_size : 32
gpu_copy_streams : 1
gpu_inference_streams : 1
input_dtype : int32
input_format : linear
log_dir : /work/build/logs/2023.12.15-17.40.48
num_sort_segments : 2
offline_expected_qps : 9.5
precision : fp16
preprocessed_data_dir : /home/zhouf/mlperf_inference_data/preprocessed_data
scenario : Scenario.Offline
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='GENUINE INTEL(R) XEON(R)', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=72, threads_per_core=2): 2}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=395.271148, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=395271148000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA H100 PCIe', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=79.6474609375, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=85520809984), max_power_limit=350.0, pci_id='0x233110DE', compute_sm=90): 1})), numa_conf=None, system_id='ACC_H100')
tensor_parallelism : 1
tensor_path : build/preprocessed_data/cnn_dailymail_tokenized_gptj/input_ids_padded.npy,build/preprocessed_data/cnn_dailymail_tokenized_gptj/masked_tokens.npy,build/preprocessed_data/cnn_dailymail_tokenized_gptj/input_lengths.npy
use_fp8 : True
use_graphs : False
system_id : ACC_H100
config_name : ACC_H100_gptj_Offline
workload_setting : WorkloadSetting(HarnessType.Custom, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
use_cpu : False
use_inferentia : False
num_profiles : 1
config_ver : custom_k_99_MaxP
accuracy_level : 99%
inference_server : custom
skip_file_checks : False
power_limit : None
cpu_freq : None
&&&& RUNNING GPT_HARNESS # ./build/bin/harness_gpt
[I] Loading plugin: build/plugins/../TRTLLM/cpp/build/tensorrt_llm/plugins/libnvinfer_plugin.so
I1215 19:22:43.632396 39947 main_gpt.cc:122] Found 1 GPUs
I1215 19:22:44.950847 39947 gpt_server.cc:215] Loading 1 engine(s)
I1215 19:22:44.950899 39947 gpt_server.cc:218] Engine Path: ./build/engines/ACC_H100/gptj/Offline/gptj-Offline-gpu-b32-fp16.custom_k_99_MaxP.plan
[I] [TRT] Loaded engine size: 6177 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 18186, GPU 7164 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 18188, GPU 7236 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +6165, now: CPU 0, GPU 6165 (MiB)
I1215 19:22:56.405355 39947 gpt_server.cc:290] Engines Deserialization Completed
I1215 19:22:56.449265 39947 gpt_core.cc:64] GPTCore 0: MPI Rank - 0 at Device Id - 0
I1215 19:22:56.449379 39947 gpt_core.cc:262] Engine - Vocab size: 50401 Padded vocab size: 50401 Beam width: 4
I1215 19:22:56.457459 39947 gpt_core.cc:90] Engine - Device Memory requirements: 7545819136
I1215 19:22:56.457468 39947 gpt_core.cc:99] Engine - Total Number of Optimization Profiles: 2
I1215 19:22:56.457469 39947 gpt_core.cc:100] Engine - Number of Optimization Profiles Per Core: 2
I1215 19:22:56.457473 39947 gpt_core.cc:101] Engine - Start Index of Optimization Profiles: 0
[I] [TRT] [MS] Running engine with multi stream info
[I] [TRT] [MS] Number of aux streams is 1
[I] [TRT] [MS] Number of total worker streams is 2
[I] [TRT] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 5844, GPU 14436 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +64, now: CPU 5845, GPU 14500 (MiB)
I1215 19:22:57.903995 39947 gpt_core.cc:115] Setting Opt.Prof. to 0
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 6165 (MiB)
[I] [TRT] [MS] Running engine with multi stream info
[I] [TRT] [MS] Number of aux streams is 1
[I] [TRT] [MS] Number of total worker streams is 2
[I] [TRT] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +64, now: CPU 5946, GPU 14632 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +74, now: CPU 5946, GPU 14706 (MiB)
I1215 19:22:58.552297 39947 gpt_core.cc:115] Setting Opt.Prof. to 1
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 6165 (MiB)
[I] [TRT] Switching optimization profile from: 0 to 1. Please ensure there are no enqueued operations pending in this context prior to switching profiles
I1215 19:22:58.618880 39947 gpt_core.cc:144] Setup complete
I1215 19:22:58.618917 39947 gpt_core.cc:1026] Device 0: Warm up bypassed.
I1215 19:22:58.619328 39947 main_gpt.cc:237] Starting running actual test.
I1215 20:10:23.319298 39947 main_gpt.cc:241] Finished running actual test.
================================================
MLPerf Results Summary
================================================
SUT name : GPT SERVER MAIN
Scenario : Offline
Mode     : PerformanceOnly
Samples per second: 8.64008
Result is : VALID
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: Yes

================================================
Additional Stats
================================================
Min latency (ns)                : 6095494899
Max latency (ns)                : 2844418721091
Mean latency (ns)               : 1553265987001
50.00 percentile latency (ns)   : 1428017130734
90.00 percentile latency (ns)   : 2678516574319
95.00 percentile latency (ns)   : 2771362445035
97.00 percentile latency (ns)   : 2803956215195
99.00 percentile latency (ns)   : 2833674139703
99.90 percentile latency (ns)   : 2844418712719

================================================
Test Parameters Used
================================================
samples_per_query : 24576
target_qps : 9.5
target_latency (ns): 0
max_async_queries : 1
min_duration (ms): 600000
max_duration (ms): 0
min_query_count : 1
max_query_count : 0
qsl_rng_seed : 148687905518835231
sample_index_rng_seed : 520418551913322573
schedule_rng_seed : 811580660758947900
accuracy_log_rng_seed : 0
accuracy_log_probability : 0
accuracy_log_sampling_target : 0
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 13368

No warnings encountered during test.

No errors encountered during test.
[2023-12-15 20:10:24,783 run_harness.py:167 INFO] Result: result_samples_per_second: 8.64008, Result is VALID
[2023-12-15 20:10:24,796 harness.py:236 INFO] The harness will load 1 plugins: ['build/plugins/../TRTLLM/cpp/build/tensorrt_llm/plugins/libnvinfer_plugin.so']
[2023-12-15 20:10:24,797 generate_conf_files.py:107 INFO] Generated measurements/ entries for ACC_H100_TRT/gptj-99/Server
[2023-12-15 20:10:24,797 __init__.py:46 INFO] Running command: ./build/bin/harness_gpt --plugins="build/plugins/../TRTLLM/cpp/build/tensorrt_llm/plugins/libnvinfer_plugin.so" --logfile_outdir="/work/build/logs/2023.12.15-17.40.48/ACC_H100_TRT/gptj-99/Server" --logfile_prefix="mlperf_log_" --performance_sample_count=13368 --gpu_batch_size=32 --tensor_path="build/preprocessed_data/cnn_dailymail_tokenized_gptj/input_ids_padded.npy,build/preprocessed_data/cnn_dailymail_tokenized_gptj/masked_tokens.npy,build/preprocessed_data/cnn_dailymail_tokenized_gptj/input_lengths.npy" --use_graphs=false --use_fp8=true --gpu_inference_streams=1 --gpu_copy_streams=1 --tensor_parallelism=1 --enable_sort=false --num_sort_segments=2 --gpu_engines="./build/engines/ACC_H100/gptj/Server/gptj-Server-gpu-b32-fp16.custom_k_99_MaxP.plan" --mlperf_conf_path="build/loadgen-configs/ACC_H100_TRT/gptj-99/Server/mlperf.conf" --user_conf_path="build/loadgen-configs/ACC_H100_TRT/gptj-99/Server/user.conf" --scenario Server --model gptj
[2023-12-15 20:10:24,797 __init__.py:53 INFO] Overriding Environment
benchmark : Benchmark.GPTJ
buffer_manager_thread_count : 0
coalesced_tensor : True
data_dir : /home/zhouf/mlperf_inference_data/data
enable_sort : False
gpu_batch_size : 32
gpu_copy_streams : 1
gpu_inference_streams : 1
input_dtype : int32
input_format : linear
log_dir : /work/build/logs/2023.12.15-17.40.48
num_sort_segments : 2
precision : fp16
preprocessed_data_dir : /home/zhouf/mlperf_inference_data/preprocessed_data
scenario : Scenario.Server
server_target_qps : 7.25
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='GENUINE INTEL(R) XEON(R)', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=72, threads_per_core=2): 2}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=395.271148, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=395271148000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA H100 PCIe', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=79.6474609375, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=85520809984), max_power_limit=350.0, pci_id='0x233110DE', compute_sm=90): 1})), numa_conf=None, system_id='ACC_H100')
tensor_parallelism : 1
tensor_path : build/preprocessed_data/cnn_dailymail_tokenized_gptj/input_ids_padded.npy,build/preprocessed_data/cnn_dailymail_tokenized_gptj/masked_tokens.npy,build/preprocessed_data/cnn_dailymail_tokenized_gptj/input_lengths.npy
use_fp8 : True
use_graphs : False
system_id : ACC_H100
config_name : ACC_H100_gptj_Server
workload_setting : WorkloadSetting(HarnessType.Custom, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
use_cpu : False
use_inferentia : False
num_profiles : 1
config_ver : custom_k_99_MaxP
accuracy_level : 99%
inference_server : custom
skip_file_checks : False
power_limit : None
cpu_freq : None
&&&& RUNNING GPT_HARNESS # ./build/bin/harness_gpt
[I] Loading plugin: build/plugins/../TRTLLM/cpp/build/tensorrt_llm/plugins/libnvinfer_plugin.so
I1215 20:10:25.416662 39961 main_gpt.cc:122] Found 1 GPUs
I1215 20:10:26.682312 39961 gpt_server.cc:215] Loading 1 engine(s)
I1215 20:10:26.682363 39961 gpt_server.cc:218] Engine Path: ./build/engines/ACC_H100/gptj/Server/gptj-Server-gpu-b32-fp16.custom_k_99_MaxP.plan
[I] [TRT] Loaded engine size: 6177 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 18186, GPU 7164 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 18188, GPU 7236 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +6165, now: CPU 0, GPU 6165 (MiB)
I1215 20:10:38.099315 39961 gpt_server.cc:290] Engines Deserialization Completed
I1215 20:10:38.142030 39961 gpt_core.cc:64] GPTCore 0: MPI Rank - 0 at Device Id - 0
I1215 20:10:38.142151 39961 gpt_core.cc:262] Engine - Vocab size: 50401 Padded vocab size: 50401 Beam width: 4
I1215 20:10:38.150703 39961 gpt_core.cc:90] Engine - Device Memory requirements: 7545819136
I1215 20:10:38.150712 39961 gpt_core.cc:99] Engine - Total Number of Optimization Profiles: 2
I1215 20:10:38.150714 39961 gpt_core.cc:100] Engine - Number of Optimization Profiles Per Core: 2
I1215 20:10:38.150717 39961 gpt_core.cc:101] Engine - Start Index of Optimization Profiles: 0
[I] [TRT] [MS] Running engine with multi stream info
[I] [TRT] [MS] Number of aux streams is 1
[I] [TRT] [MS] Number of total worker streams is 2
[I] [TRT] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 5844, GPU 14438 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +64, now: CPU 5845, GPU 14502 (MiB)
I1215 20:10:39.604537 39961 gpt_core.cc:115] Setting Opt.Prof. to 0
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 6165 (MiB)
[I] [TRT] [MS] Running engine with multi stream info
[I] [TRT] [MS] Number of aux streams is 1
[I] [TRT] [MS] Number of total worker streams is 2
[I] [TRT] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 5945, GPU 14632 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +74, now: CPU 5946, GPU 14706 (MiB)
I1215 20:10:40.262821 39961 gpt_core.cc:115] Setting Opt.Prof. to 1
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 6165 (MiB)
[I] [TRT] Switching optimization profile from: 0 to 1. Please ensure there are no enqueued operations pending in this context prior to switching profiles
I1215 20:10:40.328720 39961 gpt_core.cc:144] Setup complete
I1215 20:10:40.328749 39961 gpt_core.cc:1026] Device 0: Warm up bypassed.
I1215 20:10:40.329165 39961 main_gpt.cc:237] Starting running actual test.
I1215 20:21:08.673245 39961 main_gpt.cc:241] Finished running actual test.
================================================
MLPerf Results Summary
================================================
SUT name : GPT SERVER MAIN
Scenario : Server
Mode     : PerformanceOnly
Scheduled samples per second : 7.18
Result is : INVALID
  Performance constraints satisfied : NO
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: NO
Recommendations:
 * Reduce target QPS to improve latency.
Early Stopping Result:
 * Run unsuccessful.
 * Processed 4308 queries.
 * Would need to run at least 142643 more queries,
 with the run being successful if every additional
 query were under latency.

================================================
Additional Stats
================================================
Completed samples per second    : 6.86

Min latency (ns)                : 664207288
Max latency (ns)                : 33624222213
Mean latency (ns)               : 16332894376
50.00 percentile latency (ns)   : 15085296974
90.00 percentile latency (ns)   : 29004392950
95.00 percentile latency (ns)   : 30504707600
97.00 percentile latency (ns)   : 31227296279
99.00 percentile latency (ns)   : 32198604998
99.90 percentile latency (ns)   : 33131359846

================================================
Test Parameters Used
================================================
samples_per_query : 1
target_qps : 7.25
target_latency (ns): 20000000000
max_async_queries : 0
min_duration (ms): 600000
max_duration (ms): 0
min_query_count : 100
max_query_count : 0
qsl_rng_seed : 148687905518835231
sample_index_rng_seed : 520418551913322573
schedule_rng_seed : 811580660758947900
accuracy_log_rng_seed : 0
accuracy_log_probability : 0
accuracy_log_sampling_target : 0
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 13368

No warnings encountered during test.

No errors encountered during test.
[2023-12-15 20:21:09,827 run_harness.py:167 INFO] Result: result_scheduled_samples_per_sec: 7.17833, Result is INVALID
[2023-12-15 20:21:09,846 generate_conf_files.py:107 INFO] Generated measurements/ entries for ACC_H100_TRT/resnet50/Offline
[2023-12-15 20:21:09,846 __init__.py:46 INFO] Running command: ./build/bin/harness_default --logfile_outdir="/work/build/logs/2023.12.15-17.40.48/ACC_H100_TRT/resnet50/Offline" --logfile_prefix="mlperf_log_" --performance_sample_count=2048 --gpu_res2res3_loop_count=1 --gpu_copy_streams=2 --gpu_inference_streams=1 --run_infer_on_copy_streams=false --gpu_batch_size=2048 --map_path="data_maps/imagenet/val_map.txt" --tensor_path="build/preprocessed_data/imagenet/ResNet50/int8_linear" --use_graphs=false --gpu_engines="./build/engines/ACC_H100/resnet50/Offline/resnet50-Offline-gpu-b2048-int8.lwis_k_99_MaxP.plan" --mlperf_conf_path="build/loadgen-configs/ACC_H100_TRT/resnet50/Offline/mlperf.conf" --user_conf_path="build/loadgen-configs/ACC_H100_TRT/resnet50/Offline/user.conf" --max_dlas=0 --scenario Offline --model resnet50
[2023-12-15 20:21:09,846 __init__.py:53 INFO] Overriding Environment
benchmark : Benchmark.ResNet50
buffer_manager_thread_count : 0
data_dir : /home/zhouf/mlperf_inference_data/data
gpu_batch_size : 2048
gpu_copy_streams : 2
gpu_inference_streams : 1
gpu_res2res3_loop_count : 1
input_dtype : int8
input_format : linear
log_dir : /work/build/logs/2023.12.15-17.40.48
map_path : data_maps/imagenet/val_map.txt
offline_expected_qps : 57000
precision : int8
preprocessed_data_dir : /home/zhouf/mlperf_inference_data/preprocessed_data
run_infer_on_copy_streams : False
scenario : Scenario.Offline
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='GENUINE INTEL(R) XEON(R)', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=72, threads_per_core=2): 2}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=395.271148, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=395271148000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA H100 PCIe', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=79.6474609375, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=85520809984), max_power_limit=350.0, pci_id='0x233110DE', compute_sm=90): 1})), numa_conf=None, system_id='ACC_H100')
tensor_path : build/preprocessed_data/imagenet/ResNet50/int8_linear
use_graphs : False
system_id : ACC_H100
config_name : ACC_H100_resnet50_Offline
workload_setting : WorkloadSetting(HarnessType.LWIS, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
use_cpu : False
use_inferentia : False
num_profiles : 2
config_ver : lwis_k_99_MaxP
accuracy_level : 99%
inference_server : lwis
skip_file_checks : False
power_limit : None
cpu_freq : None
&&&& RUNNING Default_Harness # ./build/bin/harness_default
[I] mlperf.conf path: build/loadgen-configs/ACC_H100_TRT/resnet50/Offline/mlperf.conf
[I] user.conf path: build/loadgen-configs/ACC_H100_TRT/resnet50/Offline/user.conf
Creating QSL.
Finished Creating QSL.
Setting up SUT.
[I] [TRT] Loaded engine size: 31 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +7, GPU +66, now: CPU 98, GPU 1022 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 100, GPU 1094 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +19, now: CPU 0, GPU 19 (MiB)
[I] Device:0.GPU: [0] ./build/engines/ACC_H100/resnet50/Offline/resnet50-Offline-gpu-b2048-int8.lwis_k_99_MaxP.plan has been successfully loaded.
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +66, now: CPU 69, GPU 1030 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 69, GPU 1094 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +3185, now: CPU 0, GPU 3204 (MiB)
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 80, GPU 4356 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +72, now: CPU 81, GPU 4428 (MiB)
[I] [TRT] Could not set default profile 0 for execution context. Profile index must be set explicitly.
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +3185, now: CPU 0, GPU 6389 (MiB)
[I] Creating batcher thread: 0 EnableBatcherThreadPerDevice: false
Finished setting up SUT.
Starting warmup. Running for a minimum of 5 seconds.
Finished warmup. Ran for 5.51834s.
Starting running actual test.
================================================
MLPerf Results Summary
================================================
SUT name : LWIS_Server
Scenario : Offline
Mode     : PerformanceOnly
Samples per second: 51818.2
Result is : VALID
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: Yes

================================================
Additional Stats
================================================
Min latency (ns)                : 679668585
Max latency (ns)                : 725999700628
Mean latency (ns)               : 362738969733
50.00 percentile latency (ns)   : 362712673849
90.00 percentile latency (ns)   : 653261447937
95.00 percentile latency (ns)   : 689536508002
97.00 percentile latency (ns)   : 704121883195
99.00 percentile latency (ns)   : 718759237347
99.90 percentile latency (ns)   : 725279652080

================================================
Test Parameters Used
================================================
samples_per_query : 37620000
target_qps : 57000
target_latency (ns): 0
max_async_queries : 1
min_duration (ms): 600000
max_duration (ms): 0
min_query_count : 1
max_query_count : 0
qsl_rng_seed : 148687905518835231
sample_index_rng_seed : 520418551913322573
schedule_rng_seed : 811580660758947900
accuracy_log_rng_seed : 0
accuracy_log_probability : 0
accuracy_log_sampling_target : 0
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 2048

1 warning encountered. See detailed log.

No errors encountered during test.
Finished running actual test.
Device Device:0.GPU processed:
  1 batches of size 288
  18369 batches of size 2048
  Memcpy Calls: 0
  PerSampleCudaMemcpy Calls: 0
  BatchedCudaMemcpy Calls: 18370
&&&& PASSED Default_Harness # ./build/bin/harness_default
[2023-12-15 20:33:32,213 run_harness.py:167 INFO] Result: result_samples_per_second: 51818.2, Result is VALID
[2023-12-15 20:33:32,226 generate_conf_files.py:107 INFO] Generated measurements/ entries for ACC_H100_TRT/resnet50/Server
[2023-12-15 20:33:32,226 harness.py:313 INFO] Updated LD_PRELOAD: /usr/lib/x86_64-linux-gnu/libjemalloc.so.2
[2023-12-15 20:33:32,226 __init__.py:46 INFO] Running command: ./build/bin/harness_default --logfile_outdir="/work/build/logs/2023.12.15-17.40.48/ACC_H100_TRT/resnet50/Server" --logfile_prefix="mlperf_log_" --performance_sample_count=2048 --deque_timeout_usec=2000 --gpu_copy_streams=4 --gpu_inference_streams=2 --use_batcher_thread_per_device=true --use_cuda_thread_per_device=true --use_deque_limit=true --gpu_batch_size=128 --map_path="data_maps/imagenet/val_map.txt" --tensor_path="build/preprocessed_data/imagenet/ResNet50/int8_linear" --use_graphs=true --gpu_engines="./build/engines/ACC_H100/resnet50/Server/resnet50-Server-gpu-b128-int8.lwis_k_99_MaxP.plan" --mlperf_conf_path="build/loadgen-configs/ACC_H100_TRT/resnet50/Server/mlperf.conf" --user_conf_path="build/loadgen-configs/ACC_H100_TRT/resnet50/Server/user.conf" --max_dlas=0 --scenario Server --model resnet50
[2023-12-15 20:33:32,226 __init__.py:53 INFO] Overriding Environment
benchmark : Benchmark.ResNet50
buffer_manager_thread_count : 0
data_dir : /home/zhouf/mlperf_inference_data/data
deque_timeout_usec : 2000
gpu_batch_size : 128
gpu_copy_streams : 4
gpu_inference_streams : 2
input_dtype : int8
input_format : linear
log_dir : /work/build/logs/2023.12.15-17.40.48
map_path : data_maps/imagenet/val_map.txt
precision : int8
preprocessed_data_dir : /home/zhouf/mlperf_inference_data/preprocessed_data
scenario : Scenario.Server
server_target_qps : 47000
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='GENUINE INTEL(R) XEON(R)', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=72, threads_per_core=2): 2}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=395.271148, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=395271148000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA H100 PCIe', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=79.6474609375, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=85520809984), max_power_limit=350.0, pci_id='0x233110DE', compute_sm=90): 1})), numa_conf=None, system_id='ACC_H100')
tensor_path : build/preprocessed_data/imagenet/ResNet50/int8_linear
use_batcher_thread_per_device : True
use_cuda_thread_per_device : True
use_deque_limit : True
use_graphs : True
system_id : ACC_H100
config_name : ACC_H100_resnet50_Server
workload_setting : WorkloadSetting(HarnessType.LWIS, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
use_cpu : False
use_inferentia : False
num_profiles : 4
config_ver : lwis_k_99_MaxP
accuracy_level : 99%
inference_server : lwis
skip_file_checks : False
power_limit : None
cpu_freq : None
&&&& RUNNING Default_Harness # ./build/bin/harness_default
[I] mlperf.conf path: build/loadgen-configs/ACC_H100_TRT/resnet50/Server/mlperf.conf
[I] user.conf path: build/loadgen-configs/ACC_H100_TRT/resnet50/Server/user.conf
Creating QSL.
Finished Creating QSL.
Setting up SUT.
[I] [TRT] Loaded engine size: 41 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +66, now: CPU 0, GPU 1022 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +72, now: CPU 0, GPU 1094 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +19, now: CPU 0, GPU 19 (MiB)
[I] Device:0.GPU: [0] ./build/engines/ACC_H100/resnet50/Server/resnet50-Server-gpu-b128-int8.lwis_k_99_MaxP.plan has been successfully loaded.
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +66, now: CPU 0, GPU 1030 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 0, GPU 1094 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +199, now: CPU 0, GPU 218 (MiB)
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 0, GPU 1380 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +74, now: CPU 0, GPU 1454 (MiB)
[I] [TRT] Could not set default profile 0 for execution context. Profile index must be set explicitly.
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +199, now: CPU 0, GPU 417 (MiB)
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 0, GPU 1742 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +70, now: CPU 0, GPU 1812 (MiB)
[I] [TRT] Could not set default profile 0 for execution context. Profile index must be set explicitly.
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +199, now: CPU 1, GPU 616 (MiB)
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 0, GPU 2102 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +74, now: CPU 0, GPU 2176 (MiB)
[I] [TRT] Could not set default profile 0 for execution context. Profile index must be set explicitly.
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +199, now: CPU 1, GPU 815 (MiB)
[I] Start creating CUDA graphs
[I] Capture 512 CUDA graphs
[I] Finish creating CUDA graphs
[I] Creating batcher thread: 0 EnableBatcherThreadPerDevice: true
[I] Creating cuda thread: 0
Finished setting up SUT.
Starting warmup. Running for a minimum of 5 seconds.
Finished warmup. Ran for 5.20894s.
Starting running actual test.
================================================
MLPerf Results Summary
================================================
SUT name : LWIS_Server
Scenario : Server
Mode     : PerformanceOnly
Scheduled samples per second : 47017.48
Result is : INVALID
  Performance constraints satisfied : NO
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: NO
Recommendations:
 * Reduce target QPS to improve latency.
Early Stopping Result:
 * Run unsuccessful.
 * Processed 28210491 queries.
 * Would need to run at least 652162749 more queries,
 with the run being successful if every additional
 query were under latency.

================================================
Additional Stats
================================================
Completed samples per second    : 47015.27

Min latency (ns)                : 5537238
Max latency (ns)                : 225404996
Mean latency (ns)               : 22699733
50.00 percentile latency (ns)   : 11971279
90.00 percentile latency (ns)   : 49173070
95.00 percentile latency (ns)   : 96521157
97.00 percentile latency (ns)   : 123546265
99.00 percentile latency (ns)   : 155320022
99.90 percentile latency (ns)   : 219184139

================================================
Test Parameters Used
================================================
samples_per_query : 1
target_qps : 47000
target_latency (ns): 15000000
max_async_queries : 0
min_duration (ms): 600000
max_duration (ms): 0
min_query_count : 100
max_query_count : 0
qsl_rng_seed : 148687905518835231
sample_index_rng_seed : 520418551913322573
schedule_rng_seed : 811580660758947900
accuracy_log_rng_seed : 0
accuracy_log_probability : 0
accuracy_log_sampling_target : 0
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 2048

No warnings encountered during test.

No errors encountered during test.
Finished running actual test.
Device Device:0.GPU processed:
  1 batches of size 11
  1 batches of size 39
  1 batches of size 62
  1 batches of size 65
  3 batches of size 67
  4 batches of size 68
  6 batches of size 69
  10 batches of size 70
  13 batches of size 71
  21 batches of size 72
  27 batches of size 73
  31 batches of size 74
  46 batches of size 75
  55 batches of size 76
  73 batches of size 77
  70 batches of size 78
  87 batches of size 79
  134 batches of size 80
  135 batches of size 81
  168 batches of size 82
  223 batches of size 83
  275 batches of size 84
  264 batches of size 85
  359 batches of size 86
  422 batches of size 87
  445 batches of size 88
  475 batches of size 89
  590 batches of size 90
  582 batches of size 91
  681 batches of size 92
  727 batches of size 93
  799 batches of size 94
  788 batches of size 95
  945 batches of size 96
  977 batches of size 97
  1004 batches of size 98
  1067 batches of size 99
  1134 batches of size 100
  1157 batches of size 101
  1174 batches of size 102
  1224 batches of size 103
  1201 batches of size 104
  1315 batches of size 105
  1284 batches of size 106
  1301 batches of size 107
  1276 batches of size 108
  1312 batches of size 109
  1292 batches of size 110
  1337 batches of size 111
  1264 batches of size 112
  1274 batches of size 113
  1352 batches of size 114
  1279 batches of size 115
  1341 batches of size 116
  1248 batches of size 117
  1320 batches of size 118
  1291 batches of size 119
  1307 batches of size 120
  1249 batches of size 121
  1270 batches of size 122
  1233 batches of size 123
  1184 batches of size 124
  1194 batches of size 125
  1183 batches of size 126
  1179 batches of size 127
  180932 batches of size 128
  Memcpy Calls: 0
  PerSampleCudaMemcpy Calls: 28210491
  BatchedCudaMemcpy Calls: 0
&&&& PASSED Default_Harness # ./build/bin/harness_default
[2023-12-15 20:44:14,990 run_harness.py:167 INFO] Result: result_scheduled_samples_per_sec: 47017.5, Result is INVALID
[2023-12-15 20:44:15,005 harness.py:236 INFO] The harness will load 1 plugins: ['build/plugins/RNNTOptPlugin/librnntoptplugin.so']
[2023-12-15 20:44:15,006 generate_conf_files.py:107 INFO] Generated measurements/ entries for ACC_H100_TRT/rnnt/Offline
[2023-12-15 20:44:15,006 __init__.py:46 INFO] Running command: ./build/bin/harness_rnnt --plugins="build/plugins/RNNTOptPlugin/librnntoptplugin.so" --logfile_outdir="/work/build/logs/2023.12.15-17.40.48/ACC_H100_TRT/rnnt/Offline" --logfile_prefix="mlperf_log_" --performance_sample_count=2513 --audio_batch_size=512 --audio_buffer_num_lines=4096 --dali_batches_issue_ahead=4 --dali_pipeline_depth=4 --disable_encoder_plugin=false --num_warmups=512 --mlperf_conf_path="build/loadgen-configs/ACC_H100_TRT/rnnt/Offline/mlperf.conf" --user_conf_path="build/loadgen-configs/ACC_H100_TRT/rnnt/Offline/user.conf" --batch_size=2048 --cuda_graph=true --pipelined_execution=true --batch_sorting=true --enable_audio_processing=true --use_copy_kernel=true --streams_per_gpu=1 --audio_fp16_input=true --start_from_device=false --audio_serialized_pipeline_file="build/bin/dali/dali_pipeline_gpu_fp16.pth" --scenario Offline --model rnnt --engine_dir="./build/engines/ACC_H100/rnnt/Offline"
[2023-12-15 20:44:15,006 __init__.py:53 INFO] Overriding Environment
audio_batch_size : 512
audio_buffer_num_lines : 4096
benchmark : Benchmark.RNNT
buffer_manager_thread_count : 0
dali_batches_issue_ahead : 4
dali_pipeline_depth : 4
data_dir : /home/zhouf/mlperf_inference_data/data
disable_encoder_plugin : False
gpu_batch_size : 2048
gpu_copy_streams : 1
gpu_inference_streams : 1
input_dtype : fp16
input_format : linear
log_dir : /work/build/logs/2023.12.15-17.40.48
map_path : data_maps/rnnt_dev_clean_512/val_map.txt
num_warmups : 512
offline_expected_qps : 17000
precision : fp16
preprocessed_data_dir : /home/zhouf/mlperf_inference_data/preprocessed_data
scenario : Scenario.Offline
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='GENUINE INTEL(R) XEON(R)', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=72, threads_per_core=2): 2}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=395.271148, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=395271148000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA H100 PCIe', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=79.6474609375, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=85520809984), max_power_limit=350.0, pci_id='0x233110DE', compute_sm=90): 1})), numa_conf=None, system_id='ACC_H100')
tensor_path : build/preprocessed_data/rnnt_dev_clean_512/fp16
use_graphs : True
system_id : ACC_H100
config_name : ACC_H100_rnnt_Offline
workload_setting : WorkloadSetting(HarnessType.Custom, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
use_cpu : False
use_inferentia : False
num_profiles : 1
config_ver : custom_k_99_MaxP
accuracy_level : 99%
inference_server : custom
skip_file_checks : False
power_limit : None
cpu_freq : None
&&&& RUNNING RNN-T_Harness # ./build/bin/harness_rnnt
I1215 20:44:15.120414 39991 main_rnnt.cc:2903] Found 1 GPUs
[I] Starting creating QSL.
[I] Finished creating QSL.
[I] Starting creating SUT.
[I] Set to device 0
Dali pipeline creating..
Dali pipeline created
[I] Creating stream 0/1
[I] [TRT] Loaded engine size: 81 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 213, GPU 3898 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 215, GPU 3970 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 215, GPU 4082 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +64, now: CPU 216, GPU 4146 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +4174, now: CPU 0, GPU 4174 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntEncoder runner: encoder
[I] [TRT] Loaded engine size: 3 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 222, GPU 8388 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +72, now: CPU 222, GPU 8460 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 4174 (MiB)
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 222, GPU 8464 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +64, now: CPU 223, GPU 8528 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +18, now: CPU 0, GPU 4192 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntDecoder runner: decoder
[I] [TRT] Loaded engine size: 1 MiB
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +1, now: CPU 0, GPU 4193 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 4193 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntJointFc1 runner: fc1_a
[I] [TRT] Loaded engine size: 0 MiB
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +1, now: CPU 0, GPU 4194 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 4194 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntJointFc1 runner: fc1_b
[I] [TRT] Loaded engine size: 0 MiB
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 4194 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +2, now: CPU 0, GPU 4196 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntJointBackend runner: joint_backend
[I] [TRT] Loaded engine size: 0 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 224, GPU 8618 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +72, now: CPU 225, GPU 8690 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 4196 (MiB)
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 224, GPU 8626 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +64, now: CPU 225, GPU 8690 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 4196 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntIsel runner: isel
[I] [TRT] Loaded engine size: 0 MiB
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 4196 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 4196 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntIgather runner: igather
[I] Instantiated RnntEngineContainer runner
cudaMemcpy blocking 
cudaMemcpy blocking 
[I] Instantiated RnntTensorContainer host memory
Stream::Stream sampleSize: 61440
Stream::Stream singleSampleSize: 480
Stream::Stream fullseqSampleSize: 61440
Stream::Stream mBatchSize: 2048
[I] Finished creating SUT.
[I] Starting warming up SUT.
[I] Finished warming up SUT.
[I] Starting running actual test.
================================================
MLPerf Results Summary
================================================
SUT name : RNNT SERVER
Scenario : Offline
Mode     : PerformanceOnly
Samples per second: 16840.6
Result is : VALID
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: Yes

================================================
Additional Stats
================================================
Min latency (ns)                : 2480106687
Max latency (ns)                : 666245490812
Mean latency (ns)               : 421813234149
50.00 percentile latency (ns)   : 463051169381
90.00 percentile latency (ns)   : 636826759905
95.00 percentile latency (ns)   : 652002710404
97.00 percentile latency (ns)   : 657892476173
99.00 percentile latency (ns)   : 663567817620
99.90 percentile latency (ns)   : 665978146077

================================================
Test Parameters Used
================================================
samples_per_query : 11220000
target_qps : 17000
target_latency (ns): 0
max_async_queries : 1
min_duration (ms): 600000
max_duration (ms): 0
min_query_count : 1
max_query_count : 0
qsl_rng_seed : 148687905518835231
sample_index_rng_seed : 520418551913322573
schedule_rng_seed : 811580660758947900
accuracy_log_rng_seed : 0
accuracy_log_probability : 0
accuracy_log_sampling_target : 0
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 2513

1 warning encountered. See detailed log.

No errors encountered during test.
[I] Finished running actual test.
&&&& PASSED RNN-T_Harness # ./build/bin/harness_rnnt
[2023-12-15 20:55:34,718 run_harness.py:167 INFO] Result: result_samples_per_second: 16840.6, Result is VALID
[2023-12-15 20:55:34,729 harness.py:236 INFO] The harness will load 1 plugins: ['build/plugins/RNNTOptPlugin/librnntoptplugin.so']
[2023-12-15 20:55:34,730 generate_conf_files.py:107 INFO] Generated measurements/ entries for ACC_H100_TRT/rnnt/Server
[2023-12-15 20:55:34,730 __init__.py:46 INFO] Running command: ./build/bin/harness_rnnt --plugins="build/plugins/RNNTOptPlugin/librnntoptplugin.so" --logfile_outdir="/work/build/logs/2023.12.15-17.40.48/ACC_H100_TRT/rnnt/Server" --logfile_prefix="mlperf_log_" --performance_sample_count=2513 --audio_batch_size=512 --audio_buffer_num_lines=8192 --audio_fp16_input=true --dali_batches_issue_ahead=0 --dali_pipeline_depth=2 --num_warmups=20480 --mlperf_conf_path="build/loadgen-configs/ACC_H100_TRT/rnnt/Server/mlperf.conf" --user_conf_path="build/loadgen-configs/ACC_H100_TRT/rnnt/Server/user.conf" --batch_size=2048 --cuda_graph=true --pipelined_execution=true --batch_sorting=false --enable_audio_processing=true --use_copy_kernel=true --streams_per_gpu=1 --start_from_device=false --audio_serialized_pipeline_file="build/bin/dali/dali_pipeline_gpu_fp16.pth" --scenario Server --model rnnt --engine_dir="./build/engines/ACC_H100/rnnt/Server"
[2023-12-15 20:55:34,730 __init__.py:53 INFO] Overriding Environment
audio_batch_size : 512
audio_buffer_num_lines : 8192
audio_fp16_input : True
benchmark : Benchmark.RNNT
buffer_manager_thread_count : 0
dali_batches_issue_ahead : 0
dali_pipeline_depth : 2
data_dir : /home/zhouf/mlperf_inference_data/data
gpu_batch_size : 2048
gpu_copy_streams : 1
gpu_inference_streams : 1
input_dtype : fp16
input_format : linear
log_dir : /work/build/logs/2023.12.15-17.40.48
map_path : data_maps/rnnt_dev_clean_512/val_map.txt
nobatch_sorting : True
num_warmups : 20480
precision : fp16
preprocessed_data_dir : /home/zhouf/mlperf_inference_data/preprocessed_data
scenario : Scenario.Server
server_target_qps : 15000
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='GENUINE INTEL(R) XEON(R)', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=72, threads_per_core=2): 2}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=395.271148, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=395271148000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA H100 PCIe', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=79.6474609375, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=85520809984), max_power_limit=350.0, pci_id='0x233110DE', compute_sm=90): 1})), numa_conf=None, system_id='ACC_H100')
tensor_path : build/preprocessed_data/rnnt_dev_clean_512/fp16
use_graphs : True
system_id : ACC_H100
config_name : ACC_H100_rnnt_Server
workload_setting : WorkloadSetting(HarnessType.Custom, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
use_cpu : False
use_inferentia : False
num_profiles : 1
config_ver : custom_k_99_MaxP
accuracy_level : 99%
inference_server : custom
skip_file_checks : False
power_limit : None
cpu_freq : None
&&&& RUNNING RNN-T_Harness # ./build/bin/harness_rnnt
I1215 20:55:34.847522 40004 main_rnnt.cc:2903] Found 1 GPUs
[I] Starting creating QSL.
[I] Finished creating QSL.
[I] Starting creating SUT.
[I] Set to device 0
Dali pipeline creating..
Dali pipeline created
[I] Creating stream 0/1
[I] [TRT] Loaded engine size: 81 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 213, GPU 3896 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 215, GPU 3968 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 216, GPU 4080 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 216, GPU 4144 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +4174, now: CPU 0, GPU 4174 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntEncoder runner: encoder
[I] [TRT] Loaded engine size: 3 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 222, GPU 8386 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +72, now: CPU 223, GPU 8458 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 4174 (MiB)
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 223, GPU 8462 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 223, GPU 8526 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +18, now: CPU 0, GPU 4192 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntDecoder runner: decoder
[I] [TRT] Loaded engine size: 1 MiB
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +1, now: CPU 0, GPU 4193 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 4193 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntJointFc1 runner: fc1_a
[I] [TRT] Loaded engine size: 0 MiB
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +1, now: CPU 0, GPU 4194 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 4194 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntJointFc1 runner: fc1_b
[I] [TRT] Loaded engine size: 0 MiB
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 4194 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +2, now: CPU 0, GPU 4196 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntJointBackend runner: joint_backend
[I] [TRT] Loaded engine size: 0 MiB
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 225, GPU 8616 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +72, now: CPU 225, GPU 8688 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 4196 (MiB)
[I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 225, GPU 8624 (MiB)
[I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 225, GPU 8688 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 4196 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntIsel runner: isel
[I] [TRT] Loaded engine size: 0 MiB
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 4196 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 4196 (MiB)
[E] [TRT] 3: [runtime.cpp::~Runtime::399] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::399, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[I] Created RnntIgather runner: igather
[I] Instantiated RnntEngineContainer runner
cudaMemcpy blocking 
cudaMemcpy blocking 
[I] Instantiated RnntTensorContainer host memory
Stream::Stream sampleSize: 61440
Stream::Stream singleSampleSize: 480
Stream::Stream fullseqSampleSize: 61440
Stream::Stream mBatchSize: 2048
[I] Finished creating SUT.
[I] Starting warming up SUT.
[I] Finished warming up SUT.
[I] Starting running actual test.
================================================
MLPerf Results Summary
================================================
SUT name : RNNT SERVER
Scenario : Server
Mode     : PerformanceOnly
Scheduled samples per second : 15006.83
Result is : VALID
  Performance constraints satisfied : Yes
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: Yes
Early Stopping Result:
 * Run successful.

================================================
Additional Stats
================================================
Completed samples per second    : 15001.83

Min latency (ns)                : 40636603
Max latency (ns)                : 983710507
Mean latency (ns)               : 373916279
50.00 percentile latency (ns)   : 358509596
90.00 percentile latency (ns)   : 588331925
95.00 percentile latency (ns)   : 639851236
97.00 percentile latency (ns)   : 671622562
99.00 percentile latency (ns)   : 725909878
99.90 percentile latency (ns)   : 806062620

================================================
Test Parameters Used
================================================
samples_per_query : 1
target_qps : 15000
target_latency (ns): 1000000000
max_async_queries : 0
min_duration (ms): 600000
max_duration (ms): 0
min_query_count : 100
max_query_count : 0
qsl_rng_seed : 148687905518835231
sample_index_rng_seed : 520418551913322573
schedule_rng_seed : 811580660758947900
accuracy_log_rng_seed : 0
accuracy_log_probability : 0
accuracy_log_sampling_target : 0
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 2513

1 warning encountered. See detailed log.

No errors encountered during test.
[I] Finished running actual test.
&&&& PASSED RNN-T_Harness # ./build/bin/harness_rnnt
[2023-12-15 21:06:35,539 run_harness.py:167 INFO] Result: result_scheduled_samples_per_sec: 15006.8, Result is VALID
 
======================== Result summaries: ========================

 ACC_H100_TRT-custom_k_99_MaxP-Offline:
   bert-99:
     performance: result_samples_per_second: 5343.95, Result is VALID
   gptj-99:
     performance: result_samples_per_second: 8.64008, Result is VALID
   rnnt:
     performance: result_samples_per_second: 16840.6, Result is VALID
 
 ACC_H100_TRT-custom_k_99_MaxP-Server:
   bert-99:
     performance: result_scheduled_samples_per_sec: 4564.11, Result is INVALID
   gptj-99:
     performance: result_scheduled_samples_per_sec: 7.17833, Result is INVALID
   rnnt:
     performance: result_scheduled_samples_per_sec: 15006.8, Result is VALID
 
 ACC_H100_TRT-lwis_k_99_MaxP-Offline:
   resnet50:
     performance: result_samples_per_second: 51818.2, Result is VALID
 
 ACC_H100_TRT-lwis_k_99_MaxP-Server:
   resnet50:
     performance: result_scheduled_samples_per_sec: 47017.5, Result is INVALID
 

======================== Extra Perf Stats: ========================

 ACC_H100_TRT-custom_k_99_MaxP-Offline:
    FileNotFoundError: Cannot find perf logs for ACC_H100_TRT/bert-99/Offline at build/artifacts/closed/NVIDIA/results/ACC_H100_TRT/bert-99/Offline/performance/run_1. Non-NVIDIA users ignore this. NVIDIA users run `make pull_artifacts_repo`.

======================== Extra Perf Stats: ========================

 ACC_H100_TRT-custom_k_99_MaxP-Server:
    FileNotFoundError: Cannot find perf logs for ACC_H100_TRT/bert-99/Server at build/artifacts/closed/NVIDIA/results/ACC_H100_TRT/bert-99/Server/performance/run_1. Non-NVIDIA users ignore this. NVIDIA users run `make pull_artifacts_repo`.
    Server 99-percentile latency 30259692453 ns is 232.77 of the target_latency 130000000 ns

======================== Extra Perf Stats: ========================

 ACC_H100_TRT-custom_k_99_MaxP-Offline:
    FileNotFoundError: Cannot find perf logs for ACC_H100_TRT/gptj-99/Offline at build/artifacts/closed/NVIDIA/results/ACC_H100_TRT/gptj-99/Offline/performance/run_1. Non-NVIDIA users ignore this. NVIDIA users run `make pull_artifacts_repo`.

======================== Extra Perf Stats: ========================

 ACC_H100_TRT-custom_k_99_MaxP-Server:
    FileNotFoundError: Cannot find perf logs for ACC_H100_TRT/gptj-99/Server at build/artifacts/closed/NVIDIA/results/ACC_H100_TRT/gptj-99/Server/performance/run_1. Non-NVIDIA users ignore this. NVIDIA users run `make pull_artifacts_repo`.
    Server 99-percentile latency 32198604998 ns is 1.61 of the target_latency 20000000000 ns

======================== Extra Perf Stats: ========================

 ACC_H100_TRT-lwis_k_99_MaxP-Offline:
    FileNotFoundError: Cannot find perf logs for ACC_H100_TRT/resnet50/Offline at build/artifacts/closed/NVIDIA/results/ACC_H100_TRT/resnet50/Offline/performance/run_1. Non-NVIDIA users ignore this. NVIDIA users run `make pull_artifacts_repo`.

======================== Extra Perf Stats: ========================

 ACC_H100_TRT-lwis_k_99_MaxP-Server:
    FileNotFoundError: Cannot find perf logs for ACC_H100_TRT/resnet50/Server at build/artifacts/closed/NVIDIA/results/ACC_H100_TRT/resnet50/Server/performance/run_1. Non-NVIDIA users ignore this. NVIDIA users run `make pull_artifacts_repo`.
    Server 99-percentile latency 155320022 ns is 10.35 of the target_latency 15000000 ns

======================== Extra Perf Stats: ========================

 ACC_H100_TRT-custom_k_99_MaxP-Offline:
    FileNotFoundError: Cannot find perf logs for ACC_H100_TRT/rnnt/Offline at build/artifacts/closed/NVIDIA/results/ACC_H100_TRT/rnnt/Offline/performance/run_1. Non-NVIDIA users ignore this. NVIDIA users run `make pull_artifacts_repo`.

======================== Extra Perf Stats: ========================

 ACC_H100_TRT-custom_k_99_MaxP-Server:
    FileNotFoundError: Cannot find perf logs for ACC_H100_TRT/rnnt/Server at build/artifacts/closed/NVIDIA/results/ACC_H100_TRT/rnnt/Server/performance/run_1. Non-NVIDIA users ignore this. NVIDIA users run `make pull_artifacts_repo`.
    Server 99-percentile latency 725909878 ns is 0.73 of the target_latency 1000000000 ns
make[1]: Leaving directory '/work'
